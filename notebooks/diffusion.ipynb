{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf133feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H5 path: F:\\20251115\\spectra_h5\\common_proteoform.h5\n",
      "Output dir: F:\\20251130\\diffusion_out\n",
      "Device: cpu | USE_AMP: False\n",
      "Loaded dataset 'ms2_lib' from H5.\n",
      "Full shape: (778, 1600)\n",
      "Randomly selected 778 spectra for training.\n",
      "\n",
      "========== START TRAINING ==========\n",
      "\n",
      "Device: cpu\n",
      "Total spectra in file: 778\n",
      "Using spectra for training: 778\n",
      "Batch size: 256\n",
      "Steps per epoch: 3\n",
      "Timesteps (T): 10\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "===== Epoch 1/10 =====\n",
      "[Epoch 1] Batch 1/3 | global step 0\n",
      "Epoch 1 done. Mean loss: 0.361129\n",
      "\n",
      "===== Epoch 2/10 =====\n",
      "[Epoch 2] Batch 1/3 | global step 3\n",
      "Epoch 2 done. Mean loss: 0.040752\n",
      "\n",
      "===== Epoch 3/10 =====\n",
      "[Epoch 3] Batch 1/3 | global step 6\n",
      "Epoch 3 done. Mean loss: 0.020088\n",
      "\n",
      "===== Epoch 4/10 =====\n",
      "[Epoch 4] Batch 1/3 | global step 9\n",
      "Epoch 4 done. Mean loss: 0.010105\n",
      "\n",
      "===== Epoch 5/10 =====\n",
      "[Epoch 5] Batch 1/3 | global step 12\n",
      "Epoch 5 done. Mean loss: 0.011852\n",
      "\n",
      "===== Epoch 6/10 =====\n",
      "[Epoch 6] Batch 1/3 | global step 15\n",
      "Epoch 6 done. Mean loss: 0.007189\n",
      "\n",
      "===== Epoch 7/10 =====\n",
      "[Epoch 7] Batch 1/3 | global step 18\n",
      "Epoch 7 done. Mean loss: 0.006669\n",
      "\n",
      "===== Epoch 8/10 =====\n",
      "[Epoch 8] Batch 1/3 | global step 21\n",
      "Epoch 8 done. Mean loss: 0.005295\n",
      "\n",
      "===== Epoch 9/10 =====\n",
      "[Epoch 9] Batch 1/3 | global step 24\n",
      "Epoch 9 done. Mean loss: 0.004424\n",
      "\n",
      "===== Epoch 10/10 =====\n",
      "[Epoch 10] Batch 1/3 | global step 27\n",
      "Epoch 10 done. Mean loss: 0.004102\n",
      "Saved final checkpoint: F:\\20251130\\diffusion_out\\diffusion_unet_final.pt\n",
      "Saved loss plot to: F:\\20251130\\diffusion_out\\loss_curve_unet.png\n",
      "Saved synthetic spectra to: F:\\20251130\\diffusion_out\\synthetic_ms2.npy\n",
      "Saved synthetic plot to: F:\\20251130\\diffusion_out\\synthetic_ms2_unet.png\n",
      "All done. Outputs saved in: F:\\20251130\\diffusion_out\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1) Imports & paths  (LOCAL NOTEBOOK VERSION)\n",
    "# ============================================\n",
    "import os\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modern AMP API (PyTorch >= 2.0)\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# ---------- EDIT THIS PART ----------\n",
    "# Full path to your local H5 databank:\n",
    "H5_PATH     = r\"F:\\20251115\\spectra_h5\\common_proteoform.h5\"   # <-- change this\n",
    "H5_DATASET  = \"ms2_lib\"                                        # dataset name in H5\n",
    "\n",
    "# Where to save outputs (check this exists or will be created):\n",
    "OUT_DIR     = r\"F:\\20251130\\diffusion_out\"                     # <-- change if you want\n",
    "\n",
    "# -----------------------------------\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "USE_AMP       = (DEVICE == \"cuda\")\n",
    "USE_TORCH_COMPILE = False   # turn ON for long runs, OFF for quick tests\n",
    "\n",
    "BATCH_SIZE    = 256         # UNet is heavier than MLP; tweak if OOM\n",
    "NUM_EPOCHS    = 10          # increase for better quality (e.g. 30+)\n",
    "LR            = 2e-4\n",
    "NUM_TIMESTEPS = 10          # can increase later to 50–100\n",
    "SPECTRUM_LEN  = 1600        # bins from 400–2000 m/z (1 amu)\n",
    "\n",
    "NUM_SAMPLES       = 32      # how many synthetic spectra to generate\n",
    "MAX_TRAIN_SPECTRA = 5120    # randomly choose this many from H5 (None = all)\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"H5 path:\", H5_PATH)\n",
    "print(\"Output dir:\", OUT_DIR)\n",
    "print(\"Device:\", DEVICE, \"| USE_AMP:\", USE_AMP)\n",
    "\n",
    "# ============================================\n",
    "# 3) Dataset (RANDOM SUBSET + improved normalization)\n",
    "# ============================================\n",
    "class MS2H5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lazily reads spectra from local H5 and returns normalized 1D spectra.\n",
    "    Normalization:\n",
    "      - per-spectrum TIC: x = x / sum(x)\n",
    "      - sqrt compression: x = sqrt(x)\n",
    "      - map [0,1] -> [-1,1] for diffusion\n",
    "    \"\"\"\n",
    "    def __init__(self, h5_path, dataset_name, max_spectra=None, seed=42):\n",
    "        super().__init__()\n",
    "        self.h5_path = h5_path\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        self.h5 = h5py.File(self.h5_path, \"r\")\n",
    "        self.ds = self.h5[self.dataset_name]\n",
    "        full_len = self.ds.shape[0]\n",
    "\n",
    "        if (max_spectra is None) or (max_spectra >= full_len):\n",
    "            self.indices = np.arange(full_len)\n",
    "        else:\n",
    "            rng = np.random.default_rng(seed=seed)\n",
    "            self.indices = rng.choice(full_len, size=max_spectra, replace=False)\n",
    "\n",
    "        self.length = len(self.indices)\n",
    "\n",
    "        print(f\"Loaded dataset '{dataset_name}' from H5.\")\n",
    "        print(\"Full shape:\", self.ds.shape)\n",
    "        print(f\"Randomly selected {self.length} spectra for training.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        x = self.ds[real_idx].astype(np.float32)   # (SPECTRUM_LEN,)\n",
    "\n",
    "        # TIC normalization\n",
    "        tic = x.sum()\n",
    "        if tic > 0:\n",
    "            x = x / tic\n",
    "\n",
    "        # sqrt compression (better dynamic range)\n",
    "        x = np.sqrt(x)\n",
    "\n",
    "        # map [0,1] -> [-1,1]\n",
    "        x = x * 2.0 - 1.0\n",
    "\n",
    "        return torch.from_numpy(x)\n",
    "\n",
    "    def close(self):\n",
    "        if self.h5 is not None:\n",
    "            self.h5.close()\n",
    "            self.h5 = None\n",
    "\n",
    "# ============================================\n",
    "# 4) Diffusion utils (x0-prediction)\n",
    "# ============================================\n",
    "def make_beta_schedule(T, beta_start=1e-4, beta_end=2e-2):\n",
    "    return torch.linspace(beta_start, beta_end, T)\n",
    "\n",
    "class Diffusion:\n",
    "    \"\"\"\n",
    "    Standard DDPM with model predicting x0 (clean spectrum).\n",
    "    \"\"\"\n",
    "    def __init__(self, T, device):\n",
    "        self.T = T\n",
    "        self.device = device\n",
    "\n",
    "        betas = make_beta_schedule(T).to(device)   # (T,)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "        self.betas = betas\n",
    "        self.alphas = alphas\n",
    "        self.alphas_cumprod = alphas_cumprod\n",
    "        self.alphas_cumprod_prev = torch.cat(\n",
    "            [torch.tensor([1.0], device=device), alphas_cumprod[:-1]], dim=0\n",
    "        )\n",
    "\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "        self.sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "        self.posterior_var = betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion q(x_t | x_0)\n",
    "        x0: (B, D) in [-1,1]\n",
    "        t:  (B,) int in [0,T-1]\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1)\n",
    "        return sqrt_alphas_cumprod_t * x0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_sample(self, model, x_t, t):\n",
    "        \"\"\"\n",
    "        One reverse step p(x_{t-1} | x_t) using model that predicts x0.\n",
    "        \"\"\"\n",
    "        betas_t = self.betas[t].view(-1, 1)\n",
    "        alphas_t = self.alphas[t].view(-1, 1)\n",
    "        alphas_cumprod_t = self.alphas_cumprod[t].view(-1, 1)\n",
    "        alphas_cumprod_prev_t = self.alphas_cumprod_prev[t].view(-1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1)\n",
    "\n",
    "        # model predicts x0 directly\n",
    "        x0_pred = model(x_t, t).clamp(-1.0, 1.0)\n",
    "\n",
    "        # posterior mean μ_t\n",
    "        posterior_mean = (\n",
    "            betas_t * torch.sqrt(alphas_cumprod_prev_t) / (1.0 - alphas_cumprod_t) * x0_pred\n",
    "            + (torch.sqrt(alphas_t) * (1.0 - alphas_cumprod_prev_t) / (1.0 - alphas_cumprod_t)) * x_t\n",
    "        )\n",
    "\n",
    "        posterior_var_t = self.posterior_var[t].view(-1, 1)\n",
    "        if (t == 0).all():\n",
    "            return posterior_mean\n",
    "\n",
    "        noise = torch.randn_like(x_t)\n",
    "        return posterior_mean + torch.sqrt(posterior_var_t) * noise\n",
    "\n",
    "    def p_sample_loop(self, model, shape):\n",
    "        \"\"\"\n",
    "        Draw x_T ~ N(0,I) and iteratively sample to x_0.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        x_t = torch.randn(shape, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            for time_step in reversed(range(self.T)):\n",
    "                t_tensor = torch.full((shape[0],), time_step, device=self.device, dtype=torch.long)\n",
    "                x_t = self.p_sample(model, x_t, t_tensor)\n",
    "        return x_t\n",
    "\n",
    "# ============================================\n",
    "# 5) Time embedding + UNet-1D model\n",
    "# ============================================\n",
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: (B,) integer timesteps\n",
    "        returns: (B, dim)\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t.float().unsqueeze(1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        return emb\n",
    "\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim, kernel_size=3, groups=8):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, padding=padding)\n",
    "        self.norm1 = nn.GroupNorm(groups, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(groups, out_ch)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.time_proj = nn.Linear(time_dim, out_ch)\n",
    "        self.skip = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"\n",
    "        x: (B, C, L)\n",
    "        t_emb: (B, time_dim)\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        # add time embedding\n",
    "        t = self.time_proj(t_emb).unsqueeze(-1)   # (B, out_ch, 1)\n",
    "        h = h + t\n",
    "        h = self.act(self.norm1(h))\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.act(self.norm2(h))\n",
    "\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class UNet1D(nn.Module):\n",
    "    \"\"\"\n",
    "    1D U-Net for spectra with time conditioning.\n",
    "    Input:  x_t (B, D), t (B,)\n",
    "    Output: x0_pred (B, D)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dim, time_dim=128, base_ch=64):\n",
    "        super().__init__()\n",
    "        self.data_dim = data_dim\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(time_dim),\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim),\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.in_conv = nn.Conv1d(1, base_ch, 3, padding=1)\n",
    "        self.enc1 = ResBlock1D(base_ch,   base_ch,   time_dim)\n",
    "        self.down1 = nn.Conv1d(base_ch,   base_ch*2, 4, stride=2, padding=1)\n",
    "\n",
    "        self.enc2 = ResBlock1D(base_ch*2, base_ch*2, time_dim)\n",
    "        self.down2 = nn.Conv1d(base_ch*2, base_ch*4, 4, stride=2, padding=1)\n",
    "\n",
    "        self.enc3 = ResBlock1D(base_ch*4, base_ch*4, time_dim)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.mid = ResBlock1D(base_ch*4, base_ch*4, time_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.up2 = nn.ConvTranspose1d(base_ch*4, base_ch*2, 4, stride=2, padding=1)\n",
    "        self.dec2 = ResBlock1D(base_ch*4, base_ch*2, time_dim)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose1d(base_ch*2, base_ch, 4, stride=2, padding=1)\n",
    "        self.dec1 = ResBlock1D(base_ch*2, base_ch, time_dim)\n",
    "\n",
    "        self.out_conv = nn.Conv1d(base_ch, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B, D) in [-1,1]\n",
    "        t: (B,) timesteps\n",
    "        \"\"\"\n",
    "        # prepare shapes\n",
    "        x = x.unsqueeze(1)   # (B,1,L)\n",
    "        t_emb = self.time_mlp(t)  # (B, time_dim)\n",
    "\n",
    "        # Encoder\n",
    "        x0 = self.in_conv(x)\n",
    "        e1 = self.enc1(x0, t_emb)\n",
    "        d1 = self.down1(e1)\n",
    "\n",
    "        e2 = self.enc2(d1, t_emb)\n",
    "        d2 = self.down2(e2)\n",
    "\n",
    "        e3 = self.enc3(d2, t_emb)\n",
    "\n",
    "        # Bottleneck\n",
    "        m = self.mid(e3, t_emb)\n",
    "\n",
    "        # Decoder\n",
    "        u2 = self.up2(m)\n",
    "        u2 = torch.cat([u2, e2], dim=1)\n",
    "        d2 = self.dec2(u2, t_emb)\n",
    "\n",
    "        u1 = self.up1(d2)\n",
    "        u1 = torch.cat([u1, e1], dim=1)\n",
    "        d1 = self.dec1(u1, t_emb)\n",
    "\n",
    "        out = self.out_conv(d1)      # (B,1,L)\n",
    "        out = out.squeeze(1)         # (B,L)\n",
    "        return out\n",
    "\n",
    "# ============================================\n",
    "# 6) Training (UNet + x0 prediction)\n",
    "# ============================================\n",
    "def train():\n",
    "    dataset = MS2H5Dataset(\n",
    "        H5_PATH,\n",
    "        H5_DATASET,\n",
    "        max_spectra=MAX_TRAIN_SPECTRA,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=(DEVICE == \"cuda\"),\n",
    "    )\n",
    "\n",
    "    model = UNet1D(SPECTRUM_LEN).to(DEVICE)\n",
    "\n",
    "    if USE_TORCH_COMPILE:\n",
    "        try:\n",
    "            model = torch.compile(model)\n",
    "            print(\"Using torch.compile for model.\")\n",
    "        except Exception as e:\n",
    "            print(\"torch.compile not used:\", e)\n",
    "\n",
    "    diffusion = Diffusion(NUM_TIMESTEPS, DEVICE)\n",
    "    scaler = GradScaler(device=\"cuda\", enabled=USE_AMP)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    loss_log = []\n",
    "    global_step = 0\n",
    "\n",
    "    print(\"\\n========== START TRAINING ==========\\n\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Total spectra in file: {dataset.ds.shape[0]:,}\")\n",
    "    print(f\"Using spectra for training: {len(dataset):,}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Steps per epoch: {len(dataloader)}\")\n",
    "    print(f\"Timesteps (T): {NUM_TIMESTEPS}\")\n",
    "    print(\"--------------------------------------\\n\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{NUM_EPOCHS} =====\")\n",
    "        epoch_loss = []\n",
    "\n",
    "        for batch_i, batch in enumerate(dataloader):\n",
    "            batch = batch.to(DEVICE, non_blocking=True)   # x0 in [-1,1]\n",
    "\n",
    "            if batch_i == 0 or (batch_i + 1) % 50 == 0:\n",
    "                print(f\"[Epoch {epoch+1}] Batch {batch_i+1}/{len(dataloader)} \"\n",
    "                      f\"| global step {global_step}\")\n",
    "\n",
    "            # sample random timesteps\n",
    "            t = torch.randint(\n",
    "                0, NUM_TIMESTEPS,\n",
    "                (batch.size(0),),\n",
    "                device=DEVICE\n",
    "            ).long()\n",
    "\n",
    "            # forward diffusion\n",
    "            noise = torch.randn_like(batch)\n",
    "            with autocast(device_type=\"cuda\", enabled=USE_AMP):\n",
    "                x_t = diffusion.q_sample(batch, t, noise)\n",
    "                x0_pred = model(x_t, t)\n",
    "                loss = nn.functional.mse_loss(x0_pred, batch)\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            loss_log.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % 200 == 0:\n",
    "                print(f\"[Epoch {epoch+1} | Step {batch_i+1}/{len(dataloader)} \"\n",
    "                      f\"| Global {global_step}] Loss: {loss.item():.6f}\")\n",
    "\n",
    "        mean_epoch_loss = float(np.mean(epoch_loss))\n",
    "        print(f\"Epoch {epoch+1} done. Mean loss: {mean_epoch_loss:.6f}\")\n",
    "\n",
    "    # Save final checkpoint\n",
    "    ckpt_path = os.path.join(OUT_DIR, \"diffusion_unet_final.pt\")\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"epoch\": NUM_EPOCHS,\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved final checkpoint: {ckpt_path}\")\n",
    "\n",
    "    # Loss curve\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(loss_log, alpha=0.8)\n",
    "    plt.xlabel(\"Training step\")\n",
    "    plt.ylabel(\"MSE loss (x0 prediction)\")\n",
    "    plt.title(\"Diffusion Training Loss (UNet-1D)\")\n",
    "    plt.tight_layout()\n",
    "    loss_plot_path = os.path.join(OUT_DIR, \"loss_curve_unet.png\")\n",
    "    plt.savefig(loss_plot_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved loss plot to:\", loss_plot_path)\n",
    "\n",
    "    dataset.close()\n",
    "    return model, diffusion, loss_log\n",
    "\n",
    "# ============================================\n",
    "# 7) Sampling + saving outputs\n",
    "# ============================================\n",
    "def sample_and_save(model, diffusion, num_samples=NUM_SAMPLES):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # generate in [-1,1]\n",
    "    samples = diffusion.p_sample_loop(\n",
    "        model,\n",
    "        shape=(num_samples, SPECTRUM_LEN)\n",
    "    )\n",
    "\n",
    "    # map [-1,1] -> [0,1]\n",
    "    samples = (samples.clamp(-1, 1) + 1.0) / 2.0\n",
    "    samples = samples.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # optional: renormalize each spectrum to max=1\n",
    "    max_vals = samples.max(axis=1, keepdims=True)\n",
    "    max_vals[max_vals == 0] = 1.0\n",
    "    samples = samples / max_vals\n",
    "\n",
    "    out_npy = os.path.join(OUT_DIR, \"synthetic_ms2.npy\")\n",
    "    np.save(out_npy, samples)\n",
    "    print(\"Saved synthetic spectra to:\", out_npy)\n",
    "\n",
    "    # quick visualization (overlay 5)\n",
    "    mz_axis = np.linspace(400, 2000, SPECTRUM_LEN, endpoint=False)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(min(5, num_samples)):\n",
    "        plt.plot(mz_axis, samples[i], alpha=0.7)\n",
    "    plt.xlabel(\"m/z\")\n",
    "    plt.ylabel(\"normalized intensity\")\n",
    "    plt.title(\"Synthetic MS2 spectra from UNet diffusion\")\n",
    "    plt.tight_layout()\n",
    "    out_png = os.path.join(OUT_DIR, \"synthetic_ms2_unet.png\")\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "    print(\"Saved synthetic plot to:\", out_png)\n",
    "\n",
    "# ============================================\n",
    "# 8) Run training + sampling\n",
    "# ============================================\n",
    "model, diffusion, loss_log = train()\n",
    "sample_and_save(model, diffusion, NUM_SAMPLES)\n",
    "print(\"All done. Outputs saved in:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a97747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 100 epochs\n",
      "[0.24618, 0.012239, 0.00564, 0.003828, 0.003121] ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to: F:/loss_curve_unet_from_log.png\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================================\n",
    "# 1) Paste your log between the \"\"\" ... \"\"\"\n",
    "# =========================================\n",
    "log_text = \"\"\"\n",
    "===== Epoch 1/100 =====\n",
    "[Epoch 1] Batch 1/12 | global step 0\n",
    "Epoch 1 done. Mean loss: 0.246180\n",
    "\n",
    "===== Epoch 2/100 =====\n",
    "[Epoch 2] Batch 1/12 | global step 12\n",
    "Epoch 2 done. Mean loss: 0.012239\n",
    "\n",
    "===== Epoch 3/100 =====\n",
    "[Epoch 3] Batch 1/12 | global step 24\n",
    "Epoch 3 done. Mean loss: 0.005640\n",
    "\n",
    "===== Epoch 4/100 =====\n",
    "[Epoch 4] Batch 1/12 | global step 36\n",
    "Epoch 4 done. Mean loss: 0.003828\n",
    "\n",
    "===== Epoch 5/100 =====\n",
    "[Epoch 5] Batch 1/12 | global step 48\n",
    "Epoch 5 done. Mean loss: 0.003121\n",
    "\n",
    "===== Epoch 6/100 =====\n",
    "[Epoch 6] Batch 1/12 | global step 60\n",
    "Epoch 6 done. Mean loss: 0.002876\n",
    "\n",
    "===== Epoch 7/100 =====\n",
    "[Epoch 7] Batch 1/12 | global step 72\n",
    "Epoch 7 done. Mean loss: 0.002752\n",
    "\n",
    "===== Epoch 8/100 =====\n",
    "[Epoch 8] Batch 1/12 | global step 84\n",
    "Epoch 8 done. Mean loss: 0.002605\n",
    "\n",
    "===== Epoch 9/100 =====\n",
    "[Epoch 9] Batch 1/12 | global step 96\n",
    "Epoch 9 done. Mean loss: 0.002556\n",
    "\n",
    "===== Epoch 10/100 =====\n",
    "[Epoch 10] Batch 1/12 | global step 108\n",
    "Epoch 10 done. Mean loss: 0.002494\n",
    "\n",
    "===== Epoch 11/100 =====\n",
    "[Epoch 11] Batch 1/12 | global step 120\n",
    "Epoch 11 done. Mean loss: 0.002437\n",
    "\n",
    "===== Epoch 12/100 =====\n",
    "[Epoch 12] Batch 1/12 | global step 132\n",
    "Epoch 12 done. Mean loss: 0.002400\n",
    "\n",
    "===== Epoch 13/100 =====\n",
    "[Epoch 13] Batch 1/12 | global step 144\n",
    "Epoch 13 done. Mean loss: 0.002351\n",
    "\n",
    "===== Epoch 14/100 =====\n",
    "[Epoch 14] Batch 1/12 | global step 156\n",
    "Epoch 14 done. Mean loss: 0.002326\n",
    "\n",
    "===== Epoch 15/100 =====\n",
    "[Epoch 15] Batch 1/12 | global step 168\n",
    "Epoch 15 done. Mean loss: 0.002277\n",
    "\n",
    "===== Epoch 16/100 =====\n",
    "[Epoch 16] Batch 1/12 | global step 180\n",
    "Epoch 16 done. Mean loss: 0.002241\n",
    "\n",
    "===== Epoch 17/100 =====\n",
    "[Epoch 17] Batch 1/12 | global step 192\n",
    "[Epoch 17 | Step 8/12 | Global 200] Loss: 0.002278\n",
    "Epoch 17 done. Mean loss: 0.002215\n",
    "\n",
    "===== Epoch 18/100 =====\n",
    "[Epoch 18] Batch 1/12 | global step 204\n",
    "Epoch 18 done. Mean loss: 0.002191\n",
    "\n",
    "===== Epoch 19/100 =====\n",
    "[Epoch 19] Batch 1/12 | global step 216\n",
    "Epoch 19 done. Mean loss: 0.002153\n",
    "\n",
    "===== Epoch 20/100 =====\n",
    "[Epoch 20] Batch 1/12 | global step 228\n",
    "Epoch 20 done. Mean loss: 0.002136\n",
    "\n",
    "===== Epoch 21/100 =====\n",
    "[Epoch 21] Batch 1/12 | global step 240\n",
    "Epoch 21 done. Mean loss: 0.002112\n",
    "\n",
    "===== Epoch 22/100 =====\n",
    "[Epoch 22] Batch 1/12 | global step 252\n",
    "Epoch 22 done. Mean loss: 0.002089\n",
    "\n",
    "===== Epoch 23/100 =====\n",
    "[Epoch 23] Batch 1/12 | global step 264\n",
    "Epoch 23 done. Mean loss: 0.002071\n",
    "\n",
    "===== Epoch 24/100 =====\n",
    "[Epoch 24] Batch 1/12 | global step 276\n",
    "Epoch 24 done. Mean loss: 0.002046\n",
    "\n",
    "===== Epoch 25/100 =====\n",
    "[Epoch 25] Batch 1/12 | global step 288\n",
    "Epoch 25 done. Mean loss: 0.002039\n",
    "\n",
    "===== Epoch 26/100 =====\n",
    "[Epoch 26] Batch 1/12 | global step 300\n",
    "Epoch 26 done. Mean loss: 0.002009\n",
    "\n",
    "===== Epoch 27/100 =====\n",
    "[Epoch 27] Batch 1/12 | global step 312\n",
    "Epoch 27 done. Mean loss: 0.001998\n",
    "\n",
    "===== Epoch 28/100 =====\n",
    "[Epoch 28] Batch 1/12 | global step 324\n",
    "Epoch 28 done. Mean loss: 0.001989\n",
    "\n",
    "===== Epoch 29/100 =====\n",
    "[Epoch 29] Batch 1/12 | global step 336\n",
    "Epoch 29 done. Mean loss: 0.001967\n",
    "\n",
    "===== Epoch 30/100 =====\n",
    "[Epoch 30] Batch 1/12 | global step 348\n",
    "Epoch 30 done. Mean loss: 0.001963\n",
    "\n",
    "===== Epoch 31/100 =====\n",
    "[Epoch 31] Batch 1/12 | global step 360\n",
    "Epoch 31 done. Mean loss: 0.001940\n",
    "\n",
    "===== Epoch 32/100 =====\n",
    "[Epoch 32] Batch 1/12 | global step 372\n",
    "Epoch 32 done. Mean loss: 0.001934\n",
    "\n",
    "===== Epoch 33/100 =====\n",
    "[Epoch 33] Batch 1/12 | global step 384\n",
    "Epoch 33 done. Mean loss: 0.001929\n",
    "\n",
    "===== Epoch 34/100 =====\n",
    "[Epoch 34] Batch 1/12 | global step 396\n",
    "[Epoch 34 | Step 4/12 | Global 400] Loss: 0.001969\n",
    "Epoch 34 done. Mean loss: 0.001916\n",
    "\n",
    "===== Epoch 35/100 =====\n",
    "[Epoch 35] Batch 1/12 | global step 408\n",
    "Epoch 35 done. Mean loss: 0.001908\n",
    "\n",
    "===== Epoch 36/100 =====\n",
    "[Epoch 36] Batch 1/12 | global step 420\n",
    "Epoch 36 done. Mean loss: 0.001898\n",
    "\n",
    "===== Epoch 37/100 =====\n",
    "[Epoch 37] Batch 1/12 | global step 432\n",
    "Epoch 37 done. Mean loss: 0.001898\n",
    "\n",
    "===== Epoch 38/100 =====\n",
    "[Epoch 38] Batch 1/12 | global step 444\n",
    "Epoch 38 done. Mean loss: 0.001891\n",
    "\n",
    "===== Epoch 39/100 =====\n",
    "[Epoch 39] Batch 1/12 | global step 456\n",
    "Epoch 39 done. Mean loss: 0.001878\n",
    "\n",
    "===== Epoch 40/100 =====\n",
    "[Epoch 40] Batch 1/12 | global step 468\n",
    "Epoch 40 done. Mean loss: 0.001873\n",
    "\n",
    "===== Epoch 41/100 =====\n",
    "[Epoch 41] Batch 1/12 | global step 480\n",
    "Epoch 41 done. Mean loss: 0.001867\n",
    "\n",
    "===== Epoch 42/100 =====\n",
    "[Epoch 42] Batch 1/12 | global step 492\n",
    "Epoch 42 done. Mean loss: 0.001870\n",
    "\n",
    "===== Epoch 43/100 =====\n",
    "[Epoch 43] Batch 1/12 | global step 504\n",
    "Epoch 43 done. Mean loss: 0.001861\n",
    "\n",
    "===== Epoch 44/100 =====\n",
    "[Epoch 44] Batch 1/12 | global step 516\n",
    "Epoch 44 done. Mean loss: 0.001854\n",
    "\n",
    "===== Epoch 45/100 =====\n",
    "[Epoch 45] Batch 1/12 | global step 528\n",
    "Epoch 45 done. Mean loss: 0.001850\n",
    "\n",
    "===== Epoch 46/100 =====\n",
    "[Epoch 46] Batch 1/12 | global step 540\n",
    "Epoch 46 done. Mean loss: 0.001851\n",
    "\n",
    "===== Epoch 47/100 =====\n",
    "[Epoch 47] Batch 1/12 | global step 552\n",
    "Epoch 47 done. Mean loss: 0.001848\n",
    "\n",
    "===== Epoch 48/100 =====\n",
    "[Epoch 48] Batch 1/12 | global step 564\n",
    "Epoch 48 done. Mean loss: 0.001845\n",
    "\n",
    "===== Epoch 49/100 =====\n",
    "[Epoch 49] Batch 1/12 | global step 576\n",
    "Epoch 49 done. Mean loss: 0.001849\n",
    "\n",
    "===== Epoch 50/100 =====\n",
    "[Epoch 50] Batch 1/12 | global step 588\n",
    "[Epoch 50 | Step 12/12 | Global 600] Loss: 0.001796\n",
    "Epoch 50 done. Mean loss: 0.001832\n",
    "\n",
    "===== Epoch 51/100 =====\n",
    "[Epoch 51] Batch 1/12 | global step 600\n",
    "Epoch 51 done. Mean loss: 0.001835\n",
    "\n",
    "===== Epoch 52/100 =====\n",
    "[Epoch 52] Batch 1/12 | global step 612\n",
    "Epoch 52 done. Mean loss: 0.001838\n",
    "\n",
    "===== Epoch 53/100 =====\n",
    "[Epoch 53] Batch 1/12 | global step 624\n",
    "Epoch 53 done. Mean loss: 0.001837\n",
    "\n",
    "===== Epoch 54/100 =====\n",
    "[Epoch 54] Batch 1/12 | global step 636\n",
    "Epoch 54 done. Mean loss: 0.001829\n",
    "\n",
    "===== Epoch 55/100 =====\n",
    "[Epoch 55] Batch 1/12 | global step 648\n",
    "Epoch 55 done. Mean loss: 0.001822\n",
    "\n",
    "===== Epoch 56/100 =====\n",
    "[Epoch 56] Batch 1/12 | global step 660\n",
    "Epoch 56 done. Mean loss: 0.001822\n",
    "\n",
    "===== Epoch 57/100 =====\n",
    "[Epoch 57] Batch 1/12 | global step 672\n",
    "Epoch 57 done. Mean loss: 0.001822\n",
    "\n",
    "===== Epoch 58/100 =====\n",
    "[Epoch 58] Batch 1/12 | global step 684\n",
    "Epoch 58 done. Mean loss: 0.001826\n",
    "\n",
    "===== Epoch 59/100 =====\n",
    "[Epoch 59] Batch 1/12 | global step 696\n",
    "Epoch 59 done. Mean loss: 0.001819\n",
    "\n",
    "===== Epoch 60/100 =====\n",
    "[Epoch 60] Batch 1/12 | global step 708\n",
    "Epoch 60 done. Mean loss: 0.001821\n",
    "\n",
    "===== Epoch 61/100 =====\n",
    "[Epoch 61] Batch 1/12 | global step 720\n",
    "Epoch 61 done. Mean loss: 0.001818\n",
    "\n",
    "===== Epoch 62/100 =====\n",
    "[Epoch 62] Batch 1/12 | global step 732\n",
    "Epoch 62 done. Mean loss: 0.001820\n",
    "\n",
    "===== Epoch 63/100 =====\n",
    "[Epoch 63] Batch 1/12 | global step 744\n",
    "Epoch 63 done. Mean loss: 0.001818\n",
    "\n",
    "===== Epoch 64/100 =====\n",
    "[Epoch 64] Batch 1/12 | global step 756\n",
    "Epoch 64 done. Mean loss: 0.001823\n",
    "\n",
    "===== Epoch 65/100 =====\n",
    "[Epoch 65] Batch 1/12 | global step 768\n",
    "Epoch 65 done. Mean loss: 0.001826\n",
    "\n",
    "===== Epoch 66/100 =====\n",
    "[Epoch 66] Batch 1/12 | global step 780\n",
    "Epoch 66 done. Mean loss: 0.001816\n",
    "\n",
    "===== Epoch 67/100 =====\n",
    "[Epoch 67] Batch 1/12 | global step 792\n",
    "[Epoch 67 | Step 8/12 | Global 800] Loss: 0.001802\n",
    "Epoch 67 done. Mean loss: 0.001812\n",
    "\n",
    "===== Epoch 68/100 =====\n",
    "[Epoch 68] Batch 1/12 | global step 804\n",
    "Epoch 68 done. Mean loss: 0.001814\n",
    "\n",
    "===== Epoch 69/100 =====\n",
    "[Epoch 69] Batch 1/12 | global step 816\n",
    "Epoch 69 done. Mean loss: 0.001813\n",
    "\n",
    "===== Epoch 70/100 =====\n",
    "[Epoch 70] Batch 1/12 | global step 828\n",
    "Epoch 70 done. Mean loss: 0.001814\n",
    "\n",
    "===== Epoch 71/100 =====\n",
    "[Epoch 71] Batch 1/12 | global step 840\n",
    "Epoch 71 done. Mean loss: 0.001812\n",
    "\n",
    "===== Epoch 72/100 =====\n",
    "[Epoch 72] Batch 1/12 | global step 852\n",
    "Epoch 72 done. Mean loss: 0.001808\n",
    "\n",
    "===== Epoch 73/100 =====\n",
    "[Epoch 73] Batch 1/12 | global step 864\n",
    "Epoch 73 done. Mean loss: 0.001807\n",
    "\n",
    "===== Epoch 74/100 =====\n",
    "[Epoch 74] Batch 1/12 | global step 876\n",
    "Epoch 74 done. Mean loss: 0.001818\n",
    "\n",
    "===== Epoch 75/100 =====\n",
    "[Epoch 75] Batch 1/12 | global step 888\n",
    "Epoch 75 done. Mean loss: 0.001808\n",
    "\n",
    "===== Epoch 76/100 =====\n",
    "[Epoch 76] Batch 1/12 | global step 900\n",
    "Epoch 76 done. Mean loss: 0.001795\n",
    "\n",
    "===== Epoch 77/100 =====\n",
    "[Epoch 77] Batch 1/12 | global step 912\n",
    "Epoch 77 done. Mean loss: 0.001802\n",
    "\n",
    "===== Epoch 78/100 =====\n",
    "[Epoch 78] Batch 1/12 | global step 924\n",
    "Epoch 78 done. Mean loss: 0.001803\n",
    "\n",
    "===== Epoch 79/100 =====\n",
    "[Epoch 79] Batch 1/12 | global step 936\n",
    "Epoch 79 done. Mean loss: 0.001799\n",
    "\n",
    "===== Epoch 80/100 =====\n",
    "[Epoch 80] Batch 1/12 | global step 948\n",
    "Epoch 80 done. Mean loss: 0.001819\n",
    "\n",
    "===== Epoch 81/100 =====\n",
    "[Epoch 81] Batch 1/12 | global step 960\n",
    "Epoch 81 done. Mean loss: 0.001808\n",
    "\n",
    "===== Epoch 82/100 =====\n",
    "[Epoch 82] Batch 1/12 | global step 972\n",
    "Epoch 82 done. Mean loss: 0.001790\n",
    "\n",
    "===== Epoch 83/100 =====\n",
    "[Epoch 83] Batch 1/12 | global step 984\n",
    "Epoch 83 done. Mean loss: 0.001804\n",
    "\n",
    "===== Epoch 84/100 =====\n",
    "[Epoch 84] Batch 1/12 | global step 996\n",
    "[Epoch 84 | Step 4/12 | Global 1000] Loss: 0.001753\n",
    "Epoch 84 done. Mean loss: 0.001798\n",
    "\n",
    "===== Epoch 85/100 =====\n",
    "[Epoch 85] Batch 1/12 | global step 1008\n",
    "Epoch 85 done. Mean loss: 0.001792\n",
    "\n",
    "===== Epoch 86/100 =====\n",
    "[Epoch 86] Batch 1/12 | global step 1020\n",
    "Epoch 86 done. Mean loss: 0.001800\n",
    "\n",
    "===== Epoch 87/100 =====\n",
    "[Epoch 87] Batch 1/12 | global step 1032\n",
    "Epoch 87 done. Mean loss: 0.001808\n",
    "\n",
    "===== Epoch 88/100 =====\n",
    "[Epoch 88] Batch 1/12 | global step 1044\n",
    "Epoch 88 done. Mean loss: 0.001812\n",
    "\n",
    "===== Epoch 89/100 =====\n",
    "[Epoch 89] Batch 1/12 | global step 1056\n",
    "Epoch 89 done. Mean loss: 0.001803\n",
    "\n",
    "===== Epoch 90/100 =====\n",
    "[Epoch 90] Batch 1/12 | global step 1068\n",
    "Epoch 90 done. Mean loss: 0.001778\n",
    "\n",
    "===== Epoch 91/100 =====\n",
    "[Epoch 91] Batch 1/12 | global step 1080\n",
    "Epoch 91 done. Mean loss: 0.001788\n",
    "\n",
    "===== Epoch 92/100 =====\n",
    "[Epoch 92] Batch 1/12 | global step 1092\n",
    "Epoch 92 done. Mean loss: 0.001774\n",
    "\n",
    "===== Epoch 93/100 =====\n",
    "[Epoch 93] Batch 1/12 | global step 1104\n",
    "Epoch 93 done. Mean loss: 0.001765\n",
    "\n",
    "===== Epoch 94/100 =====\n",
    "[Epoch 94] Batch 1/12 | global step 1116\n",
    "Epoch 94 done. Mean loss: 0.001773\n",
    "\n",
    "===== Epoch 95/100 =====\n",
    "[Epoch 95] Batch 1/12 | global step 1128\n",
    "Epoch 95 done. Mean loss: 0.001773\n",
    "\n",
    "===== Epoch 96/100 =====\n",
    "[Epoch 96] Batch 1/12 | global step 1140\n",
    "Epoch 96 done. Mean loss: 0.001780\n",
    "\n",
    "===== Epoch 97/100 =====\n",
    "[Epoch 97] Batch 1/12 | global step 1152\n",
    "Epoch 97 done. Mean loss: 0.001778\n",
    "\n",
    "===== Epoch 98/100 =====\n",
    "[Epoch 98] Batch 1/12 | global step 1164\n",
    "Epoch 98 done. Mean loss: 0.001819\n",
    "\n",
    "===== Epoch 99/100 =====\n",
    "[Epoch 99] Batch 1/12 | global step 1176\n",
    "Epoch 99 done. Mean loss: 0.001810\n",
    "\n",
    "===== Epoch 100/100 =====\n",
    "[Epoch 100] Batch 1/12 | global step 1188\n",
    "[Epoch 100 | Step 12/12 | Global 1200] Loss: 0.001760\n",
    "Epoch 100 done. Mean loss: 0.001780\n",
    "\"\"\"\n",
    "\n",
    "# =========================================\n",
    "# 2) Extract mean losses from the log\n",
    "# =========================================\n",
    "losses = [float(x) for x in re.findall(r\"Mean loss:\\s*([0-9.]+)\", log_text)]\n",
    "epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "print(f\"Parsed {len(losses)} epochs\")\n",
    "print(losses[:5], \"...\")\n",
    "\n",
    "# =========================================\n",
    "# 3) Plot loss vs epoch\n",
    "# =========================================\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, losses, marker='o', linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean training loss\")\n",
    "plt.title(\"Diffusion UNet – Loss vs Epoch\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale(\"log\")  # log scale is usually nice for these curves\n",
    "\n",
    "# Save next to your other outputs\n",
    "out_path = \"F:/loss_curve_unet_from_log.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved plot to:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
