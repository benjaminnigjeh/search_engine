{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82522e44",
   "metadata": {},
   "source": [
    "end to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52fda0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Configured LABEL rank range: 1 .. 11 (1-based, inclusive)\n",
      "MIN_SPECTRA_PER_LABEL = 100\n",
      "MAX_SPECTRA_PER_LABEL = 1000\n",
      "\n",
      "Loading CSV: F:\\20251201\\ms2_with_ids.csv\n",
      "Found 1600 MS2 columns: ['cast_00000', 'cast_00001', 'cast_00002', 'cast_00003', 'cast_00004'] ...\n",
      "Total rows in CSV: 19307\n",
      "Rows with non-empty Accession (used for supervised training): 4485\n",
      "Any NaN left in ms2_lib? False\n",
      "Any inf left in ms2_lib? False\n",
      "Spectra shape before per-label construction: (N, n_bins) = (4485, 1600)\n",
      "Example raw labels (Accession): ['P05204' 'Q9Y2I7-2' 'P02686-5' 'P02686-5' 'P02686-5']\n",
      "\n",
      "Applying per-label limits:\n",
      "Labels dropped for having < 100 spectra: 58\n",
      "Labels capped at 1000 spectra: 1\n",
      "Total spectra after per-label MIN/MAX filter: 3343\n",
      "Spectra shape after per-label MIN/MAX: (N, n_bins) = (3343, 1600)\n",
      "Total unique labels before rank-frequency filter: 11\n",
      "\n",
      "Using LABEL ranks 1..11 (1-based).\n",
      "Number of LABEL classes in this block: 11\n",
      "Spectra after LABEL-block (rank) filter: 3343\n",
      "Number of unique LABELs in block: 11\n",
      "Final spectra shape after LABEL block filter: (N, n_bins) = (3343, 1600)\n",
      "\n",
      "Training on ALL spectra after filtering: 3343\n",
      "\n",
      "steps_per_epoch: 26\n",
      "Running a sanity check on one batch...\n",
      "Any NaN in emb_ex? False\n",
      "Initial loss (sanity check): 2.5935397148132324\n",
      "\n",
      "Starting training (contrastive spectrum encoder)...\n",
      "[Spectrum] Epoch 1/50  Avg loss: 0.8050\n",
      "[Spectrum] Epoch 2/50  Avg loss: 0.3577\n",
      "[Spectrum] Epoch 3/50  Avg loss: 0.2104\n",
      "[Spectrum] Epoch 4/50  Avg loss: 0.2351\n",
      "[Spectrum] Epoch 5/50  Avg loss: 0.1571\n",
      "[Spectrum] Epoch 6/50  Avg loss: 0.1713\n",
      "[Spectrum] Epoch 7/50  Avg loss: 0.1276\n",
      "[Spectrum] Epoch 8/50  Avg loss: 0.1952\n",
      "[Spectrum] Epoch 9/50  Avg loss: 0.1125\n",
      "[Spectrum] Epoch 10/50  Avg loss: 0.1405\n",
      "[Spectrum] Epoch 11/50  Avg loss: 0.1489\n",
      "[Spectrum] Epoch 12/50  Avg loss: 0.1066\n",
      "[Spectrum] Epoch 13/50  Avg loss: 0.0850\n",
      "[Spectrum] Epoch 14/50  Avg loss: 0.1096\n",
      "[Spectrum] Epoch 15/50  Avg loss: 0.0667\n",
      "[Spectrum] Epoch 16/50  Avg loss: 0.1033\n",
      "[Spectrum] Epoch 17/50  Avg loss: 0.0942\n",
      "[Spectrum] Epoch 18/50  Avg loss: 0.0670\n",
      "[Spectrum] Epoch 19/50  Avg loss: 0.0909\n",
      "[Spectrum] Epoch 20/50  Avg loss: 0.0897\n",
      "[Spectrum] Epoch 21/50  Avg loss: 0.1013\n",
      "[Spectrum] Epoch 22/50  Avg loss: 0.0909\n",
      "[Spectrum] Epoch 23/50  Avg loss: 0.0892\n",
      "[Spectrum] Epoch 24/50  Avg loss: 0.0574\n",
      "[Spectrum] Epoch 25/50  Avg loss: 0.0814\n",
      "[Spectrum] Epoch 26/50  Avg loss: 0.0844\n",
      "[Spectrum] Epoch 27/50  Avg loss: 0.0672\n",
      "[Spectrum] Epoch 28/50  Avg loss: 0.0702\n",
      "[Spectrum] Epoch 29/50  Avg loss: 0.0632\n",
      "[Spectrum] Epoch 30/50  Avg loss: 0.0523\n",
      "[Spectrum] Epoch 31/50  Avg loss: 0.1074\n",
      "[Spectrum] Epoch 32/50  Avg loss: 0.0857\n",
      "[Spectrum] Epoch 33/50  Avg loss: 0.0671\n",
      "[Spectrum] Epoch 34/50  Avg loss: 0.0990\n",
      "[Spectrum] Epoch 35/50  Avg loss: 0.0727\n",
      "[Spectrum] Epoch 36/50  Avg loss: 0.0721\n",
      "[Spectrum] Epoch 37/50  Avg loss: 0.0600\n",
      "[Spectrum] Epoch 38/50  Avg loss: 0.1244\n",
      "[Spectrum] Epoch 39/50  Avg loss: 0.0645\n",
      "[Spectrum] Epoch 40/50  Avg loss: 0.0385\n",
      "[Spectrum] Epoch 41/50  Avg loss: 0.0466\n",
      "[Spectrum] Epoch 42/50  Avg loss: 0.0502\n",
      "[Spectrum] Epoch 43/50  Avg loss: 0.0494\n",
      "[Spectrum] Epoch 44/50  Avg loss: 0.0408\n",
      "[Spectrum] Epoch 45/50  Avg loss: 0.0558\n",
      "[Spectrum] Epoch 46/50  Avg loss: 0.0760\n",
      "[Spectrum] Epoch 47/50  Avg loss: 0.0633\n",
      "[Spectrum] Epoch 48/50  Avg loss: 0.0999\n",
      "[Spectrum] Epoch 49/50  Avg loss: 0.0843\n",
      "[Spectrum] Epoch 50/50  Avg loss: 0.0438\n",
      "Contrastive training done.\n",
      "Spectrum encoder losses per epoch: [0.8050352598612125, 0.3577142260395564, 0.21037670604598063, 0.23507230213055244, 0.15706126452781832, 0.1712997042072507, 0.1275564597095721, 0.195197503010814, 0.11248657791517101, 0.14048575395001814, 0.14887068173489892, 0.10659745211999577, 0.08497959488214782, 0.10964423213870479, 0.0667486498132348, 0.10329158602354045, 0.09421903482423379, 0.06704334049181153, 0.09089194641161996, 0.08966487045220745, 0.10129051625083846, 0.09086582789985606, 0.08919152581634429, 0.057414821577437505, 0.0813548544045681, 0.08435382392221633, 0.06715128698404162, 0.07016173740311597, 0.06320239275317782, 0.05229461381364112, 0.10737926802203919, 0.0856935792029477, 0.0670657528792687, 0.09904906425911647, 0.07272214065484989, 0.07213365651505928, 0.0600266597533706, 0.12435058127784242, 0.06454783529401399, 0.03852800835291138, 0.04662444537881619, 0.050150392084525756, 0.04944236471111743, 0.04079749496080554, 0.05576058441343216, 0.07599817817726244, 0.06329239972938712, 0.09990386843072394, 0.08430561455539785, 0.04384955712325441]\n",
      "\n",
      "Train embeddings shape: (3343, 64)\n",
      "Contrastive spectrum model saved to contrastive_spectrum_encoder_Accession_min100_max1000_ranks_1_11.pth\n",
      "Spectrum embedding index saved to train_spectrum_embeddings.npz\n",
      "\n",
      "Per-Accession targets for PLM:\n",
      "  #labels with both sequence and embedding: 11\n",
      "  target_embs shape: (11, 64)\n",
      "\n",
      "Loading protein language model: facebook/esm2_t6_8M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting PLM fine-tuning to predict MS2 embeddings...\n",
      "[PLM] Epoch 1/20  Avg loss: 1.0679\n",
      "[PLM] Epoch 2/20  Avg loss: 1.0262\n",
      "[PLM] Epoch 3/20  Avg loss: 1.0163\n",
      "[PLM] Epoch 4/20  Avg loss: 1.0206\n",
      "[PLM] Epoch 5/20  Avg loss: 1.0074\n",
      "[PLM] Epoch 6/20  Avg loss: 0.9925\n",
      "[PLM] Epoch 7/20  Avg loss: 0.9848\n",
      "[PLM] Epoch 8/20  Avg loss: 0.9405\n",
      "[PLM] Epoch 9/20  Avg loss: 0.9546\n",
      "[PLM] Epoch 10/20  Avg loss: 0.9310\n",
      "[PLM] Epoch 11/20  Avg loss: 0.9325\n",
      "[PLM] Epoch 12/20  Avg loss: 0.9047\n",
      "[PLM] Epoch 13/20  Avg loss: 0.8893\n",
      "[PLM] Epoch 14/20  Avg loss: 0.8957\n",
      "[PLM] Epoch 15/20  Avg loss: 0.8416\n",
      "[PLM] Epoch 16/20  Avg loss: 0.8724\n",
      "[PLM] Epoch 17/20  Avg loss: 0.8416\n",
      "[PLM] Epoch 18/20  Avg loss: 0.8390\n",
      "[PLM] Epoch 19/20  Avg loss: 0.8855\n",
      "[PLM] Epoch 20/20  Avg loss: 0.7956\n",
      "PLM fine-tuning done.\n",
      "PLM regressor losses per epoch: [1.067890703678131, 1.0261692106723785, 1.0162689983844757, 1.0205867886543274, 1.007427841424942, 0.9924833178520203, 0.9847925007343292, 0.9405037462711334, 0.9545674324035645, 0.9310449659824371, 0.9325042963027954, 0.9046848714351654, 0.8892689645290375, 0.8956965208053589, 0.8416272401809692, 0.8724181950092316, 0.8415816724300385, 0.8390438556671143, 0.8854986429214478, 0.7955591380596161]\n",
      "Protein→MS2 embedding model saved to protein_to_ms2_plm_facebook_esm2_t6_8M_UR50D_emb64_ranks_1_11.pth\n",
      "\n",
      "Test sequence embedding shape: (64,)\n",
      "First 10 dims: [ 0.03937895  0.13559431  0.08321279  0.3215883   0.12338505 -0.00040992\n",
      "  0.11497311  0.01315822 -0.05754223  0.01564885]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# =========================\n",
    "# 0. Config\n",
    "# =========================\n",
    "CSV_PATH        = r\"F:\\20251201\\ms2_with_ids.csv\"  # <-- path to your CSV\n",
    "DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "EMB_DIM         = 64           # embedding dimension for MS2 contrastive encoder\n",
    "BATCH_P_CLASSES = 32           # number of labels per batch\n",
    "BATCH_K_SPECTRA = 4            # spectra per label per batch\n",
    "EPOCHS          = 50           # MS2 contrastive training epochs\n",
    "LR              = 1e-3\n",
    "TEMPERATURE     = 0.07\n",
    "\n",
    "RANDOM_SEED     = 42\n",
    "\n",
    "# ---- per-label controls ----\n",
    "MAX_SPECTRA_PER_LABEL = 1000    # hard cap per label\n",
    "MIN_SPECTRA_PER_LABEL = 100     # drop labels with fewer than this\n",
    "\n",
    "# ---- configurable label frequency rank range (1-based) ----\n",
    "START_RANK      = 1            # e.g. 1\n",
    "END_RANK        = 11           # e.g. 11 (match your ranks_1_11 checkpoint if you want)\n",
    "\n",
    "# ---- PLM fine-tuning config ----\n",
    "PLM_NAME        = \"facebook/esm2_t6_8M_UR50D\"\n",
    "MAX_SEQ_LEN     = 1024\n",
    "PLM_BATCH_SIZE  = 8\n",
    "PLM_EPOCHS      = 20\n",
    "PLM_LR          = 1e-4\n",
    "FREEZE_BACKBONE = True  # set False to fine-tune backbone as well\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(f\"Configured LABEL rank range: {START_RANK} .. {END_RANK} (1-based, inclusive)\")\n",
    "print(f\"MIN_SPECTRA_PER_LABEL = {MIN_SPECTRA_PER_LABEL}\")\n",
    "print(f\"MAX_SPECTRA_PER_LABEL = {MAX_SPECTRA_PER_LABEL}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. Load CSV & initial clean\n",
    "# =========================\n",
    "print(\"\\nLoading CSV:\", CSV_PATH)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Identify MS2 columns: cast_0000..cast_01599\n",
    "ms2_cols = [col for col in df.columns if col.startswith(\"cast_\")]\n",
    "if len(ms2_cols) == 0:\n",
    "    raise ValueError(\"No columns starting with 'cast_' found in CSV.\")\n",
    "\n",
    "ms2_cols = sorted(ms2_cols)  # ensure correct order cast_0000, cast_0001, ...\n",
    "\n",
    "print(f\"Found {len(ms2_cols)} MS2 columns:\", ms2_cols[:5], \"...\")\n",
    "\n",
    "# Labels from 'Accession'\n",
    "if \"Accession\" not in df.columns:\n",
    "    raise ValueError(\"CSV does not contain 'Accession' column.\")\n",
    "\n",
    "accession_raw = df[\"Accession\"]\n",
    "\n",
    "# Keep only rows with non-empty Accession (identified)\n",
    "mask_identified = accession_raw.notna() & (accession_raw.astype(str).str.strip() != \"\")\n",
    "df_id = df[mask_identified].reset_index(drop=True)\n",
    "accession = df_id[\"Accession\"].astype(str).str.strip().values\n",
    "\n",
    "print(\"Total rows in CSV:\", len(df))\n",
    "print(\"Rows with non-empty Accession (used for supervised training):\", len(df_id))\n",
    "\n",
    "if len(df_id) == 0:\n",
    "    raise ValueError(\"No rows with non-empty Accession. Cannot run supervised contrastive training.\")\n",
    "\n",
    "# Extract MS2 intensities\n",
    "ms2_lib = df_id[ms2_cols].values.astype(np.float32)\n",
    "\n",
    "# Replace NaN/inf in ms2 with 0\n",
    "bad_mask = ~np.isfinite(ms2_lib)\n",
    "if bad_mask.any():\n",
    "    print(\"Found NaN/inf in ms2_lib, replacing with 0\")\n",
    "    ms2_lib[bad_mask] = 0.0\n",
    "\n",
    "# Per-spectrum max normalization\n",
    "max_intensity = np.max(ms2_lib, axis=1, keepdims=True)\n",
    "max_intensity[max_intensity == 0] = 1.0\n",
    "ms2_lib = ms2_lib / max_intensity\n",
    "\n",
    "print(\"Any NaN left in ms2_lib?\", np.isnan(ms2_lib).any())\n",
    "print(\"Any inf left in ms2_lib?\", np.isinf(ms2_lib).any())\n",
    "\n",
    "N_total, n_bins = ms2_lib.shape\n",
    "print(f\"Spectra shape before per-label construction: (N, n_bins) = {ms2_lib.shape}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Build label = Accession string and enforce MIN/MAX per label\n",
    "# =========================\n",
    "label_str = np.array(accession, dtype=object)\n",
    "print(\"Example raw labels (Accession):\", label_str[:5])\n",
    "\n",
    "print(\"\\nApplying per-label limits:\")\n",
    "label_to_indices = defaultdict(list)\n",
    "for i, lab in enumerate(label_str):\n",
    "    label_to_indices[lab].append(i)\n",
    "\n",
    "keep_indices = []\n",
    "dropped_too_few = 0\n",
    "capped_too_many = 0\n",
    "\n",
    "for lab, idxs in label_to_indices.items():\n",
    "    n = len(idxs)\n",
    "    if n < MIN_SPECTRA_PER_LABEL:\n",
    "        dropped_too_few += 1\n",
    "        continue  # drop this label entirely\n",
    "    if n > MAX_SPECTRA_PER_LABEL:\n",
    "        capped_too_many += 1\n",
    "        chosen = np.random.choice(idxs, size=MAX_SPECTRA_PER_LABEL, replace=False)\n",
    "        keep_indices.extend(chosen.tolist())\n",
    "    else:\n",
    "        keep_indices.extend(idxs)\n",
    "\n",
    "keep_indices = np.array(keep_indices, dtype=int)\n",
    "\n",
    "print(f\"Labels dropped for having < {MIN_SPECTRA_PER_LABEL} spectra: {dropped_too_few}\")\n",
    "print(f\"Labels capped at {MAX_SPECTRA_PER_LABEL} spectra: {capped_too_many}\")\n",
    "print(\"Total spectra after per-label MIN/MAX filter:\", len(keep_indices))\n",
    "\n",
    "if len(keep_indices) == 0:\n",
    "    raise ValueError(\n",
    "        \"No spectra left after applying MIN_SPECTRA_PER_LABEL and MAX_SPECTRA_PER_LABEL. \"\n",
    "        \"Relax thresholds or check dataset.\"\n",
    "    )\n",
    "\n",
    "# Apply per-label filter\n",
    "ms2_lib   = ms2_lib[keep_indices]\n",
    "label_str = label_str[keep_indices]\n",
    "\n",
    "N_total, n_bins = ms2_lib.shape\n",
    "print(f\"Spectra shape after per-label MIN/MAX: (N, n_bins) = {ms2_lib.shape}\")\n",
    "\n",
    "# Keep metadata aligned (including sequence)\n",
    "df_id_filtered = df_id.iloc[keep_indices].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. Restrict to chosen LABEL rank range (frequency-based)\n",
    "# =========================\n",
    "all_unique_lbl, all_lbl_ids = np.unique(label_str, return_inverse=True)\n",
    "counts = np.bincount(all_lbl_ids)\n",
    "print(\"Total unique labels before rank-frequency filter:\", len(all_unique_lbl))\n",
    "\n",
    "# Sort labels by frequency (descending)\n",
    "sorted_class_indices = np.argsort(-counts)  # 0..(n_classes-1), high→low frequency\n",
    "n_classes_total = len(sorted_class_indices)\n",
    "\n",
    "# Clip requested rank range to available classes\n",
    "start_rank = max(1, START_RANK)\n",
    "end_rank   = min(END_RANK, n_classes_total)\n",
    "\n",
    "if end_rank < start_rank:\n",
    "    raise ValueError(\n",
    "        f\"No valid LABEL ranks after clipping to available classes. \"\n",
    "        f\"Requested {START_RANK}-{END_RANK}, available 1-{n_classes_total}.\"\n",
    "    )\n",
    "\n",
    "start_idx = start_rank - 1  # inclusive\n",
    "end_idx   = end_rank        # exclusive in slicing\n",
    "\n",
    "block_indices = sorted_class_indices[start_idx:end_idx]\n",
    "print(f\"\\nUsing LABEL ranks {start_rank}..{end_rank} (1-based).\")\n",
    "print(f\"Number of LABEL classes in this block: {len(block_indices)}\")\n",
    "\n",
    "if len(block_indices) == 0:\n",
    "    raise ValueError(\n",
    "        \"No LABEL classes in the requested rank range. \"\n",
    "        \"Try a different range (e.g. 1..50).\"\n",
    "    )\n",
    "\n",
    "block_set = set(block_indices.tolist())\n",
    "\n",
    "# Filter spectra to only these labels\n",
    "mask_block        = np.isin(all_lbl_ids, list(block_set))\n",
    "label_str_block   = label_str[mask_block]\n",
    "ms2_lib_block     = ms2_lib[mask_block]\n",
    "lbl_ids_block_raw = all_lbl_ids[mask_block]\n",
    "\n",
    "print(\"Spectra after LABEL-block (rank) filter:\", len(label_str_block))\n",
    "\n",
    "# Align metadata for this block\n",
    "df_block = df_id_filtered[mask_block].reset_index(drop=True)\n",
    "\n",
    "# Re-encode LABELs compactly 0..(K-1)\n",
    "unique_lbl_block, lbl_ids_block = np.unique(label_str_block, return_inverse=True)\n",
    "unique_lbl_block = np.array(unique_lbl_block, dtype=object)\n",
    "n_classes = len(unique_lbl_block)\n",
    "print(\"Number of unique LABELs in block:\", n_classes)\n",
    "\n",
    "N, n_bins = ms2_lib_block.shape\n",
    "print(f\"Final spectra shape after LABEL block filter: (N, n_bins) = {ms2_lib_block.shape}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. Use ALL filtered spectra as training data (NO test split)\n",
    "# =========================\n",
    "train_ms2   = ms2_lib_block\n",
    "train_lblid = lbl_ids_block\n",
    "\n",
    "print(f\"\\nTraining on ALL spectra after filtering: {len(train_ms2)}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. Dataset & balanced sampler\n",
    "# =========================\n",
    "class SpectraDataset(Dataset):\n",
    "    def __init__(self, ms2, labels):\n",
    "        self.ms2    = torch.tensor(ms2,   dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ms2.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"spectrum\": self.ms2[idx],\n",
    "            \"label\":    self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "class ClassBalancedBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Samples batches with:\n",
    "      - classes_per_batch distinct labels\n",
    "      - samples_per_class spectra for each label\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, classes_per_batch=32, samples_per_class=4):\n",
    "        self.labels = np.array(labels, dtype=np.int64)\n",
    "        self.classes_per_batch = classes_per_batch\n",
    "        self.samples_per_class = samples_per_class\n",
    "\n",
    "        self.class_to_indices = defaultdict(list)\n",
    "        for i, lab in enumerate(self.labels):\n",
    "            self.class_to_indices[lab].append(i)\n",
    "\n",
    "        self.unique_classes = np.array(list(self.class_to_indices.keys()))\n",
    "        self.batch_size = self.classes_per_batch * self.samples_per_class\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if len(self.unique_classes) <= self.classes_per_batch:\n",
    "                chosen_classes = self.unique_classes\n",
    "            else:\n",
    "                chosen_classes = np.random.choice(\n",
    "                    self.unique_classes,\n",
    "                    size=self.classes_per_batch,\n",
    "                    replace=False\n",
    "                )\n",
    "\n",
    "            batch_indices = []\n",
    "            for c in chosen_classes:\n",
    "                idxs = self.class_to_indices[c]\n",
    "                if len(idxs) >= self.samples_per_class:\n",
    "                    chosen = np.random.choice(idxs, size=self.samples_per_class, replace=False)\n",
    "                else:\n",
    "                    chosen = np.random.choice(idxs, size=self.samples_per_class, replace=True)\n",
    "                batch_indices.extend(chosen.tolist())\n",
    "            yield batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        # approximate number of batches per epoch\n",
    "        return max(1, len(self.labels) // self.batch_size)\n",
    "\n",
    "\n",
    "train_dataset = SpectraDataset(train_ms2, train_lblid)\n",
    "train_sampler = ClassBalancedBatchSampler(\n",
    "    labels=train_lblid,\n",
    "    classes_per_batch=BATCH_P_CLASSES,\n",
    "    samples_per_class=BATCH_K_SPECTRA\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_sampler\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6. Model: Spectrum Encoder\n",
    "# =========================\n",
    "class SpectrumEncoder(nn.Module):\n",
    "    def __init__(self, n_bins, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_bins, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        if torch.isnan(z).any():\n",
    "            print(\"Warning: NaN detected in encoder output, applying nan_to_num.\")\n",
    "            z = torch.nan_to_num(z, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7. Safe supervised contrastive loss\n",
    "# =========================\n",
    "def supervised_contrastive_loss_safe(emb, labels, temperature=0.07):\n",
    "    \"\"\"\n",
    "    emb    : (B, D) L2-normalized embeddings\n",
    "    labels : (B,) int label ids\n",
    "    \"\"\"\n",
    "    device = emb.device\n",
    "    B, D = emb.shape\n",
    "    if B <= 1:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    # Cosine similarity\n",
    "    sim = emb @ emb.t()\n",
    "    sim = torch.clamp(sim / temperature, min=-50.0, max=50.0)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    valid_anchors = 0\n",
    "\n",
    "    for i in range(B):\n",
    "        sim_i = sim[i].clone()\n",
    "        sim_i[i] = float(\"-inf\")\n",
    "\n",
    "        pos_mask_i = (labels == labels[i]) & (torch.arange(B, device=device) != i)\n",
    "        if not pos_mask_i.any():\n",
    "            continue\n",
    "\n",
    "        finite_mask = torch.isfinite(sim_i)\n",
    "        if not finite_mask.any():\n",
    "            continue\n",
    "\n",
    "        max_sim = sim_i[finite_mask].max()\n",
    "        shifted = sim_i - max_sim\n",
    "        shifted[~finite_mask] = float(\"-inf\")\n",
    "\n",
    "        exp_all = torch.exp(shifted)\n",
    "        denom = exp_all.sum()\n",
    "        if not torch.isfinite(denom) or denom <= 0:\n",
    "            continue\n",
    "\n",
    "        exp_pos = exp_all * pos_mask_i.float()\n",
    "        num = exp_pos.sum()\n",
    "        if not torch.isfinite(num) or num <= 0:\n",
    "            continue\n",
    "\n",
    "        loss_i = -torch.log(num / denom)\n",
    "        if not torch.isfinite(loss_i):\n",
    "            continue\n",
    "\n",
    "        total_loss += loss_i\n",
    "        valid_anchors += 1\n",
    "\n",
    "    if valid_anchors == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = total_loss / valid_anchors\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8. Training loop (contrastive spectrum encoder)\n",
    "# =========================\n",
    "model = SpectrumEncoder(n_bins=n_bins, emb_dim=EMB_DIM).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "print(\"\\nsteps_per_epoch:\", steps_per_epoch)\n",
    "\n",
    "# Sanity check\n",
    "print(\"Running a sanity check on one batch...\")\n",
    "batch_example = next(iter(train_loader))\n",
    "x_ex = batch_example[\"spectrum\"].to(DEVICE)\n",
    "y_ex = batch_example[\"label\"].to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    emb_ex = model(x_ex)\n",
    "    print(\"Any NaN in emb_ex?\", torch.isnan(emb_ex).any().item())\n",
    "    loss_ex = supervised_contrastive_loss_safe(emb_ex, y_ex, temperature=TEMPERATURE)\n",
    "    print(\"Initial loss (sanity check):\", loss_ex.item())\n",
    "\n",
    "print(\"\\nStarting training (contrastive spectrum encoder)...\")\n",
    "spectrum_epoch_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    effective_steps = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        if step >= steps_per_epoch:\n",
    "            break  # prevent infinite epoch\n",
    "\n",
    "        x = batch[\"spectrum\"].to(DEVICE)\n",
    "        y = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        emb = model(x)\n",
    "        loss = supervised_contrastive_loss_safe(emb, y, temperature=TEMPERATURE)\n",
    "\n",
    "        if torch.isnan(loss) or not torch.isfinite(loss):\n",
    "            print(f\"NaN/inf loss at epoch {epoch+1}, step {step+1}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        effective_steps += 1\n",
    "\n",
    "    avg_loss = running_loss / max(1, effective_steps)\n",
    "    spectrum_epoch_losses.append(avg_loss)\n",
    "    print(f\"[Spectrum] Epoch {epoch+1}/{EPOCHS}  Avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Contrastive training done.\")\n",
    "print(\"Spectrum encoder losses per epoch:\", spectrum_epoch_losses)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9. Build train embedding index\n",
    "# =========================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_emb = []\n",
    "    batch_size_eval = 2048\n",
    "    for i in range(0, len(train_ms2), batch_size_eval):\n",
    "        batch = torch.tensor(\n",
    "            train_ms2[i:i+batch_size_eval],\n",
    "            dtype=torch.float32,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        z = model(batch)\n",
    "        train_emb.append(z.cpu().numpy())\n",
    "    train_emb = np.concatenate(train_emb, axis=0)\n",
    "\n",
    "print(\"\\nTrain embeddings shape:\", train_emb.shape)\n",
    "train_labels = train_lblid\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 10. Save spectrum encoder model\n",
    "# =========================\n",
    "MODEL_PATH = (\n",
    "    f\"contrastive_spectrum_encoder_Accession_\"\n",
    "    f\"min{MIN_SPECTRA_PER_LABEL}_max{MAX_SPECTRA_PER_LABEL}_\"\n",
    "    f\"ranks_{start_rank}_{end_rank}.pth\"\n",
    ")\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"emb_dim\": EMB_DIM,\n",
    "    \"n_bins\": n_bins,\n",
    "    \"unique_label_block\": list(unique_lbl_block),  # accession labels for this block\n",
    "    \"rank_start\": start_rank,\n",
    "    \"rank_end\": end_rank,\n",
    "    \"min_spectra_per_label\": MIN_SPECTRA_PER_LABEL,\n",
    "    \"max_spectra_per_label\": MAX_SPECTRA_PER_LABEL,\n",
    "}, MODEL_PATH)\n",
    "\n",
    "print(f\"Contrastive spectrum model saved to {MODEL_PATH}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 11. Save spectrum embedding index for retrieval\n",
    "# =========================\n",
    "acc_per_spectrum = np.array(\n",
    "    [str(unique_lbl_block[label]) for label in train_labels],\n",
    "    dtype=object\n",
    ")\n",
    "\n",
    "INDEX_PATH = r\"train_spectrum_embeddings.npz\"\n",
    "np.savez(\n",
    "    INDEX_PATH,\n",
    "    embeddings=train_emb,         # (N, 64)\n",
    "    labels=train_labels,          # (N,)\n",
    "    accessions=acc_per_spectrum   # (N,)\n",
    ")\n",
    "print(f\"Spectrum embedding index saved to {INDEX_PATH}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 12. Build per-Accession target embeddings for PLM\n",
    "# =========================\n",
    "if \"sequence\" not in df_block.columns:\n",
    "    raise ValueError(\n",
    "        \"df_block does not have a 'sequence' column. \"\n",
    "        \"You must supply sequences in the CSV.\"\n",
    "    )\n",
    "\n",
    "acc_block = df_block[\"Accession\"].astype(str).str.strip().values\n",
    "seq_block = df_block[\"sequence\"].astype(str).values  # one sequence per spectrum row\n",
    "\n",
    "n_labels_plm = n_classes  # same as number of unique_lbl_block\n",
    "\n",
    "label_ids = []\n",
    "target_embs = []\n",
    "seqs_for_labels = []\n",
    "\n",
    "for lab_id in range(n_labels_plm):\n",
    "    acc = str(unique_lbl_block[lab_id])\n",
    "\n",
    "    # indices of spectra belonging to this label\n",
    "    idxs = np.where(train_labels == lab_id)[0]\n",
    "    if len(idxs) == 0:\n",
    "        continue\n",
    "\n",
    "    # mean spectrum embedding for this Accession (target for regression)\n",
    "    mean_emb = train_emb[idxs].mean(axis=0)\n",
    "\n",
    "    # choose a representative sequence: e.g., the longest among all spectra with this accession\n",
    "    mask_acc = (acc_block == acc)\n",
    "    seqs = seq_block[mask_acc]\n",
    "    if len(seqs) == 0:\n",
    "        continue\n",
    "\n",
    "    seq_rep = max(seqs, key=len)\n",
    "\n",
    "    label_ids.append(lab_id)\n",
    "    target_embs.append(mean_emb)\n",
    "    seqs_for_labels.append(seq_rep)\n",
    "\n",
    "label_ids = np.array(label_ids, dtype=int)\n",
    "target_embs = np.stack(target_embs, axis=0).astype(np.float32)\n",
    "\n",
    "print(f\"\\nPer-Accession targets for PLM:\")\n",
    "print(\"  #labels with both sequence and embedding:\", len(label_ids))\n",
    "print(\"  target_embs shape:\", target_embs.shape)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 13. Protein LM → MS2 embedding regressor\n",
    "# =========================\n",
    "print(\"\\nLoading protein language model:\", PLM_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PLM_NAME)\n",
    "backbone = AutoModel.from_pretrained(PLM_NAME)\n",
    "\n",
    "\n",
    "class ProteinToSpectrumRegressor(nn.Module):\n",
    "    def __init__(self, backbone, emb_dim, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        hidden_size = backbone.config.hidden_size\n",
    "        self.reg_head = nn.Linear(hidden_size, emb_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        token_embs = outputs.last_hidden_state   # (B, L, H)\n",
    "        cls = token_embs[:, 0, :]               # (B, H)\n",
    "        z = self.reg_head(cls)                  # (B, emb_dim)\n",
    "        z = F.normalize(z, dim=-1)              # keep in same manifold as train_emb\n",
    "        return z\n",
    "\n",
    "\n",
    "plm_model = ProteinToSpectrumRegressor(\n",
    "    backbone,\n",
    "    emb_dim=EMB_DIM,\n",
    "    freeze_backbone=FREEZE_BACKBONE\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "class ProteinSeqDataset(Dataset):\n",
    "    def __init__(self, sequences, targets, tokenizer, max_len=1024):\n",
    "        self.sequences = list(sequences)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        tgt = self.targets[idx]\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            seq,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        item = {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"target\": tgt\n",
    "        }\n",
    "        return item\n",
    "\n",
    "\n",
    "def cosine_regression_loss(pred, target):\n",
    "    \"\"\"\n",
    "    Encourage high cosine similarity between predicted and target embeddings.\n",
    "    pred, target: (B, D)\n",
    "    \"\"\"\n",
    "    pred = F.normalize(pred, dim=-1)\n",
    "    target = F.normalize(target, dim=-1)\n",
    "    cos = (pred * target).sum(dim=-1)  # (B,)\n",
    "    return 1.0 - cos.mean()\n",
    "\n",
    "\n",
    "plm_dataset = ProteinSeqDataset(\n",
    "    sequences=seqs_for_labels,\n",
    "    targets=target_embs,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_SEQ_LEN\n",
    ")\n",
    "\n",
    "plm_loader = DataLoader(\n",
    "    plm_dataset,\n",
    "    batch_size=PLM_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 14. Train PLM → MS2 regressor\n",
    "# =========================\n",
    "optimizer_plm = torch.optim.AdamW(plm_model.parameters(), lr=PLM_LR)\n",
    "\n",
    "print(\"\\nStarting PLM fine-tuning to predict MS2 embeddings...\")\n",
    "plm_epoch_losses = []\n",
    "for epoch in range(PLM_EPOCHS):\n",
    "    plm_model.train()\n",
    "    running_loss = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    for batch in plm_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        target = batch[\"target\"].to(DEVICE)\n",
    "\n",
    "        pred = plm_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = cosine_regression_loss(pred, target)\n",
    "\n",
    "        optimizer_plm.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_plm.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_steps += 1\n",
    "\n",
    "    avg_loss = running_loss / max(1, n_steps)\n",
    "    plm_epoch_losses.append(avg_loss)\n",
    "    print(f\"[PLM] Epoch {epoch+1}/{PLM_EPOCHS}  Avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"PLM fine-tuning done.\")\n",
    "print(\"PLM regressor losses per epoch:\", plm_epoch_losses)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 15. Save Protein→MS2 embedding model\n",
    "# =========================\n",
    "PLM_MODEL_PATH = (\n",
    "    f\"protein_to_ms2_plm_{PLM_NAME.replace('/', '_')}_\"\n",
    "    f\"emb{EMB_DIM}_ranks_{start_rank}_{end_rank}.pth\"\n",
    ")\n",
    "\n",
    "torch.save({\n",
    "    \"plm_name\": PLM_NAME,\n",
    "    \"model_state_dict\": plm_model.state_dict(),\n",
    "    \"emb_dim\": EMB_DIM,\n",
    "    \"rank_start\": start_rank,\n",
    "    \"rank_end\": end_rank,\n",
    "    \"label_ids\": label_ids.tolist(),\n",
    "    \"accessions\": [str(unique_lbl_block[i]) for i in label_ids],\n",
    "}, PLM_MODEL_PATH)\n",
    "\n",
    "print(f\"Protein→MS2 embedding model saved to {PLM_MODEL_PATH}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 16. Function: embed a sequence with trained PLM\n",
    "# =========================\n",
    "def embed_sequence(seq: str):\n",
    "    enc = tokenizer(\n",
    "        seq,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_SEQ_LEN\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    plm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = plm_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    return z[0].cpu().numpy()   # (EMB_DIM,)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 17. Quick sanity check on a test sequence\n",
    "# =========================\n",
    "test_seq = (\n",
    "    \"MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDRFKHLKTE\"\n",
    "    \"AEMKASEDLKKHGATVLTALGGILKKKGKH\"\n",
    ")\n",
    "\n",
    "z_seq = embed_sequence(test_seq)\n",
    "print(\"\\nTest sequence embedding shape:\", z_seq.shape)\n",
    "print(\"First 10 dims:\", z_seq[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeab59d",
   "metadata": {},
   "source": [
    "Training from HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f6f0377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Output directory: F:\\20251201\\result\n",
      "Configured LABEL rank range: 1 .. 11 (1-based, inclusive)\n",
      "MIN_SPECTRA_PER_LABEL = 1000\n",
      "MAX_SPECTRA_PER_LABEL = 100\n",
      "\n",
      "Loading HDF5: F:\\20251115\\spectra_h5\\combined_annotated.h5\n",
      "Keys inside the H5 file:\n",
      " - Accession\n",
      " - MASS\n",
      " - PFR\n",
      " - file_name\n",
      " - group_name\n",
      " - ms2_lib\n",
      " - precursor_mz\n",
      " - rt_min\n",
      " - scan\n",
      " - sequence\n",
      "ms2_lib shape: (2202567, 1600)\n",
      "Accession array shape: (2202567,)\n",
      "Total rows in H5: 2202567\n",
      "Rows with non-empty Accession (used for supervised training): 398060\n",
      "Any NaN left in ms2_lib? False\n",
      "Any inf left in ms2_lib? False\n",
      "Spectra shape before per-label construction: (N, n_bins) = (398060, 1600)\n",
      "Example raw labels (Accession): ['P68363-1' 'P02671-1' 'P02671-1' 'P02671-1' 'P02671-1']\n",
      "\n",
      "Applying per-label limits:\n",
      "Labels dropped for having < 1000 spectra: 1757\n",
      "Labels capped at 100 spectra: 58\n",
      "Total spectra after per-label MIN/MAX filter: 5800\n",
      "Spectra shape after per-label MIN/MAX: (N, n_bins) = (5800, 1600)\n",
      "Total unique labels before rank-frequency filter: 58\n",
      "\n",
      "Using LABEL ranks 1..11 (1-based).\n",
      "Number of LABEL classes in this block: 11\n",
      "Spectra after LABEL-block (rank) filter: 1100\n",
      "Number of unique LABELs in block: 11\n",
      "Final spectra shape after LABEL block filter: (N, n_bins) = (1100, 1600)\n",
      "\n",
      "Training on ALL spectra after filtering: 1100\n",
      "\n",
      "steps_per_epoch: 8\n",
      "Running a sanity check on one batch...\n",
      "Any NaN in emb_ex? False\n",
      "Initial loss (sanity check): 2.5495126247406006\n",
      "\n",
      "Starting training (contrastive spectrum encoder)...\n",
      "[Spectrum] Epoch 1/50  Avg loss: 1.6435\n",
      "[Spectrum] Epoch 2/50  Avg loss: 0.7821\n",
      "[Spectrum] Epoch 3/50  Avg loss: 0.6083\n",
      "[Spectrum] Epoch 4/50  Avg loss: 0.3687\n",
      "[Spectrum] Epoch 5/50  Avg loss: 0.3088\n",
      "[Spectrum] Epoch 6/50  Avg loss: 0.3937\n",
      "[Spectrum] Epoch 7/50  Avg loss: 0.2655\n",
      "[Spectrum] Epoch 8/50  Avg loss: 0.1162\n",
      "[Spectrum] Epoch 9/50  Avg loss: 0.1336\n",
      "[Spectrum] Epoch 10/50  Avg loss: 0.1781\n",
      "[Spectrum] Epoch 11/50  Avg loss: 0.2222\n",
      "[Spectrum] Epoch 12/50  Avg loss: 0.2461\n",
      "[Spectrum] Epoch 13/50  Avg loss: 0.1154\n",
      "[Spectrum] Epoch 14/50  Avg loss: 0.1424\n",
      "[Spectrum] Epoch 15/50  Avg loss: 0.0971\n",
      "[Spectrum] Epoch 16/50  Avg loss: 0.0807\n",
      "[Spectrum] Epoch 17/50  Avg loss: 0.0859\n",
      "[Spectrum] Epoch 18/50  Avg loss: 0.0584\n",
      "[Spectrum] Epoch 19/50  Avg loss: 0.0275\n",
      "[Spectrum] Epoch 20/50  Avg loss: 0.0842\n",
      "[Spectrum] Epoch 21/50  Avg loss: 0.0574\n",
      "[Spectrum] Epoch 22/50  Avg loss: 0.0692\n",
      "[Spectrum] Epoch 23/50  Avg loss: 0.0436\n",
      "[Spectrum] Epoch 24/50  Avg loss: 0.0215\n",
      "[Spectrum] Epoch 25/50  Avg loss: 0.0576\n",
      "[Spectrum] Epoch 26/50  Avg loss: 0.0151\n",
      "[Spectrum] Epoch 27/50  Avg loss: 0.0170\n",
      "[Spectrum] Epoch 28/50  Avg loss: 0.0238\n",
      "[Spectrum] Epoch 29/50  Avg loss: 0.0273\n",
      "[Spectrum] Epoch 30/50  Avg loss: 0.0178\n",
      "[Spectrum] Epoch 31/50  Avg loss: 0.0178\n",
      "[Spectrum] Epoch 32/50  Avg loss: 0.0363\n",
      "[Spectrum] Epoch 33/50  Avg loss: 0.0019\n",
      "[Spectrum] Epoch 34/50  Avg loss: 0.0058\n",
      "[Spectrum] Epoch 35/50  Avg loss: 0.0259\n",
      "[Spectrum] Epoch 36/50  Avg loss: 0.0143\n",
      "[Spectrum] Epoch 37/50  Avg loss: 0.0282\n",
      "[Spectrum] Epoch 38/50  Avg loss: 0.0115\n",
      "[Spectrum] Epoch 39/50  Avg loss: 0.0299\n",
      "[Spectrum] Epoch 40/50  Avg loss: 0.0232\n",
      "[Spectrum] Epoch 41/50  Avg loss: 0.0194\n",
      "[Spectrum] Epoch 42/50  Avg loss: 0.0170\n",
      "[Spectrum] Epoch 43/50  Avg loss: 0.0350\n",
      "[Spectrum] Epoch 44/50  Avg loss: 0.0133\n",
      "[Spectrum] Epoch 45/50  Avg loss: 0.0035\n",
      "[Spectrum] Epoch 46/50  Avg loss: 0.0360\n",
      "[Spectrum] Epoch 47/50  Avg loss: 0.0058\n",
      "[Spectrum] Epoch 48/50  Avg loss: 0.0084\n",
      "[Spectrum] Epoch 49/50  Avg loss: 0.0029\n",
      "[Spectrum] Epoch 50/50  Avg loss: 0.0042\n",
      "Contrastive training done.\n",
      "Spectrum encoder losses per epoch: [1.6434730738401413, 0.7821374945342541, 0.6082683466374874, 0.3687032740563154, 0.3087692726403475, 0.3936669621616602, 0.26552940905094147, 0.11619039345532656, 0.13360022334381938, 0.17814617231488228, 0.22220308519899845, 0.24607329349964857, 0.11544809769839048, 0.1424328834982589, 0.09714663517661393, 0.08072204841300845, 0.08591361250728369, 0.058400622103363276, 0.02753778878832236, 0.08424291951814666, 0.05744981141469907, 0.06923136883415282, 0.043609677617496345, 0.02152526451391168, 0.05764921530499123, 0.015091491906787269, 0.017045084678102285, 0.02381765507743694, 0.027263863041298464, 0.017805035968194716, 0.01777930767275393, 0.036315599660156295, 0.0018574798305053264, 0.005827942353789695, 0.025867216485494282, 0.01434604539826978, 0.02819934025683324, 0.011530684838362504, 0.029875012780394172, 0.023217998939799145, 0.019378420372959226, 0.01697264083486516, 0.035002114171220455, 0.013266170339193195, 0.0034831445809686556, 0.036024091321451124, 0.005814402691612486, 0.008442807687970344, 0.0028745504714606795, 0.004202320047625108]\n",
      "Spectrum epoch losses saved to F:\\20251201\\result\\spectrum_epoch_losses.csv\n",
      "\n",
      "Train embeddings shape: (1100, 64)\n",
      "Contrastive spectrum model saved to F:\\20251201\\result\\contrastive_spectrum_encoder_Accession_min1000_max100_ranks_1_11.pth\n",
      "Spectrum embedding index saved to F:\\20251201\\result\\train_spectrum_embeddings.npz\n",
      "\n",
      "Parsing FASTA: F:\\20251201\\human.fasta\n",
      "Found 20420 accessions with sequences in FASTA.\n",
      "\n",
      "Per-Accession targets for PLM:\n",
      "  #labels with both AA sequence and embedding: 10\n",
      "  #labels missing sequence in FASTA: 1\n",
      "  target_embs shape: (10, 64)\n",
      "\n",
      "Loading protein language model: facebook/esm2_t6_8M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting PLM fine-tuning to predict MS2 embeddings...\n",
      "[PLM] Epoch 1/20  Avg loss: 1.0140\n",
      "[PLM] Epoch 2/20  Avg loss: 0.9926\n",
      "[PLM] Epoch 3/20  Avg loss: 0.9872\n",
      "[PLM] Epoch 4/20  Avg loss: 0.9666\n",
      "[PLM] Epoch 5/20  Avg loss: 0.9367\n",
      "[PLM] Epoch 6/20  Avg loss: 1.0012\n",
      "[PLM] Epoch 7/20  Avg loss: 0.9849\n",
      "[PLM] Epoch 8/20  Avg loss: 0.8727\n",
      "[PLM] Epoch 9/20  Avg loss: 0.9211\n",
      "[PLM] Epoch 10/20  Avg loss: 0.9400\n",
      "[PLM] Epoch 11/20  Avg loss: 0.9709\n",
      "[PLM] Epoch 12/20  Avg loss: 0.9186\n",
      "[PLM] Epoch 13/20  Avg loss: 0.8627\n",
      "[PLM] Epoch 14/20  Avg loss: 0.9403\n",
      "[PLM] Epoch 15/20  Avg loss: 0.8401\n",
      "[PLM] Epoch 16/20  Avg loss: 0.8725\n",
      "[PLM] Epoch 17/20  Avg loss: 0.9275\n",
      "[PLM] Epoch 18/20  Avg loss: 0.8069\n",
      "[PLM] Epoch 19/20  Avg loss: 0.8067\n",
      "[PLM] Epoch 20/20  Avg loss: 0.8106\n",
      "PLM fine-tuning done.\n",
      "PLM regressor losses per epoch: [1.0140445232391357, 0.992554098367691, 0.9872357547283173, 0.9666179716587067, 0.9366526007652283, 1.0011983811855316, 0.9849086701869965, 0.8726727366447449, 0.9210588932037354, 0.9400357007980347, 0.9708887934684753, 0.9186097383499146, 0.8626709580421448, 0.9402570128440857, 0.8400866687297821, 0.8725059032440186, 0.9274889528751373, 0.8069155216217041, 0.8066926300525665, 0.8105930387973785]\n",
      "PLM epoch losses saved to F:\\20251201\\result\\plm_epoch_losses.csv\n",
      "Protein→MS2 embedding model saved to F:\\20251201\\result\\protein_to_ms2_plm_facebook_esm2_t6_8M_UR50D_emb64_ranks_1_11.pth\n",
      "\n",
      "Test sequence embedding shape: (64,)\n",
      "First 10 dims: [-0.00680202  0.0975192   0.12197915  0.2838466  -0.0074479  -0.10820927\n",
      " -0.07911748 -0.03392715 -0.09536437  0.10540149]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd  # for saving loss curves as CSV\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0. Config\n",
    "# =========================\n",
    "H5_PATH        = r\"F:\\20251115\\spectra_h5\\combined_annotated.h5\"\n",
    "FASTA_PATH     = r\"F:\\20251201\\human.fasta\"  # <-- UPDATE THIS TO YOUR human.fasta\n",
    "\n",
    "# Folder where ALL outputs will be saved\n",
    "OUTPUT_DIR     = r\"F:\\20251201\\result\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "EMB_DIM         = 64           # embedding dimension for MS2 contrastive encoder\n",
    "BATCH_P_CLASSES = 32           # number of labels per batch\n",
    "BATCH_K_SPECTRA = 4            # spectra per label per batch\n",
    "EPOCHS          = 50           # MS2 contrastive training epochs\n",
    "LR              = 1e-3\n",
    "TEMPERATURE     = 0.07\n",
    "\n",
    "RANDOM_SEED     = 42\n",
    "\n",
    "# ---- per-label controls ----\n",
    "MAX_SPECTRA_PER_LABEL = 100    # hard cap per label\n",
    "MIN_SPECTRA_PER_LABEL = 1000     # drop labels with fewer than this\n",
    "\n",
    "# ---- configurable label frequency rank range (1-based) ----\n",
    "START_RANK      = 1            # e.g. 1\n",
    "END_RANK        = 11           # e.g. 11\n",
    "\n",
    "# ---- PLM fine-tuning config ----\n",
    "PLM_NAME        = \"facebook/esm2_t6_8M_UR50D\"\n",
    "MAX_SEQ_LEN     = 1024\n",
    "PLM_BATCH_SIZE  = 8\n",
    "PLM_EPOCHS      = 20\n",
    "PLM_LR          = 1e-4\n",
    "FREEZE_BACKBONE = True  # set False to fine-tune backbone as well\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Configured LABEL rank range: {START_RANK} .. {END_RANK} (1-based, inclusive)\")\n",
    "print(f\"MIN_SPECTRA_PER_LABEL = {MIN_SPECTRA_PER_LABEL}\")\n",
    "print(f\"MAX_SPECTRA_PER_LABEL = {MAX_SPECTRA_PER_LABEL}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. Helpers\n",
    "# =========================\n",
    "def decode_bytes_array(arr):\n",
    "    \"\"\"Decode bytes/objects from HDF5 into Python strings.\"\"\"\n",
    "    out = []\n",
    "    for x in arr:\n",
    "        if isinstance(x, (bytes, bytearray)):\n",
    "            out.append(x.decode(\"utf-8\", errors=\"ignore\"))\n",
    "        elif x is None:\n",
    "            out.append(\"\")\n",
    "        else:\n",
    "            out.append(str(x))\n",
    "    return np.array(out, dtype=object)\n",
    "\n",
    "\n",
    "def parse_fasta_to_dict(fasta_path):\n",
    "    \"\"\"\n",
    "    Parse UniProt-like FASTA into dict: accession -> AA sequence.\n",
    "\n",
    "    Handles headers like:\n",
    "      >sp|P02679-2|FIBG_HUMAN ...\n",
    "      >tr|Q9H0H5|SOME_PROT ...\n",
    "      >P02679-2 some description\n",
    "    \"\"\"\n",
    "    acc_to_seq = {}\n",
    "    header = None\n",
    "    seq_chunks = []\n",
    "\n",
    "    def flush_entry(h, chunks):\n",
    "        if h is None or not chunks:\n",
    "            return\n",
    "        # get first token before space\n",
    "        tok0 = h.split()[0]\n",
    "        parts = tok0.split(\"|\")\n",
    "        if len(parts) == 1:\n",
    "            acc = parts[0]\n",
    "        else:\n",
    "            # UniProt: sp|ACC|NAME\n",
    "            acc = parts[1] if parts[1] else parts[0]\n",
    "        seq = \"\".join(chunks).strip().upper()\n",
    "        if acc and seq:\n",
    "            acc_to_seq[acc] = seq\n",
    "\n",
    "    with open(fasta_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                # flush previous\n",
    "                flush_entry(header, seq_chunks)\n",
    "                header = line[1:]\n",
    "                seq_chunks = []\n",
    "            else:\n",
    "                seq_chunks.append(line)\n",
    "\n",
    "        # flush last\n",
    "        flush_entry(header, seq_chunks)\n",
    "\n",
    "    return acc_to_seq\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Load HDF5 & initial clean\n",
    "# =========================\n",
    "print(\"\\nLoading HDF5:\", H5_PATH)\n",
    "with h5py.File(H5_PATH, \"r\") as h5:\n",
    "    print(\"Keys inside the H5 file:\")\n",
    "    for key in h5.keys():\n",
    "        print(\" -\", key)\n",
    "\n",
    "    # spectra\n",
    "    ms2_lib = h5[\"ms2_lib\"][:].astype(np.float32)\n",
    "\n",
    "    # labels\n",
    "    accession_raw = decode_bytes_array(h5[\"Accession\"][:])\n",
    "\n",
    "print(\"ms2_lib shape:\", ms2_lib.shape)\n",
    "print(\"Accession array shape:\", accession_raw.shape)\n",
    "\n",
    "# Keep only rows with non-empty Accession (identified)\n",
    "mask_identified = np.array(\n",
    "    [s is not None and str(s).strip() not in (\"\", \"nan\", \"None\") for s in accession_raw],\n",
    "    dtype=bool\n",
    ")\n",
    "\n",
    "ms2_lib = ms2_lib[mask_identified]\n",
    "accession = accession_raw[mask_identified]\n",
    "\n",
    "print(\"Total rows in H5:\", len(accession_raw))\n",
    "print(\"Rows with non-empty Accession (used for supervised training):\", len(accession))\n",
    "\n",
    "if len(accession) == 0:\n",
    "    raise ValueError(\"No rows with non-empty Accession. Cannot run supervised contrastive training.\")\n",
    "\n",
    "# Replace NaN/inf in ms2 with 0\n",
    "bad_mask = ~np.isfinite(ms2_lib)\n",
    "if bad_mask.any():\n",
    "    print(\"Found NaN/inf in ms2_lib, replacing with 0\")\n",
    "    ms2_lib[bad_mask] = 0.0\n",
    "\n",
    "# Per-spectrum max normalization\n",
    "max_intensity = np.max(ms2_lib, axis=1, keepdims=True)\n",
    "max_intensity[max_intensity == 0] = 1.0\n",
    "ms2_lib = ms2_lib / max_intensity\n",
    "\n",
    "print(\"Any NaN left in ms2_lib?\", np.isnan(ms2_lib).any())\n",
    "print(\"Any inf left in ms2_lib?\", np.isinf(ms2_lib).any())\n",
    "\n",
    "N_total, n_bins = ms2_lib.shape\n",
    "print(f\"Spectra shape before per-label construction: (N, n_bins) = {ms2_lib.shape}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. Build label = Accession string and enforce MIN/MAX per label\n",
    "# =========================\n",
    "label_str = np.array(accession, dtype=object)\n",
    "\n",
    "print(\"Example raw labels (Accession):\", label_str[:5])\n",
    "\n",
    "print(\"\\nApplying per-label limits:\")\n",
    "label_to_indices = defaultdict(list)\n",
    "for i, lab in enumerate(label_str):\n",
    "    label_to_indices[lab].append(i)\n",
    "\n",
    "keep_indices = []\n",
    "dropped_too_few = 0\n",
    "capped_too_many = 0\n",
    "\n",
    "for lab, idxs in label_to_indices.items():\n",
    "    n = len(idxs)\n",
    "    if n < MIN_SPECTRA_PER_LABEL:\n",
    "        dropped_too_few += 1\n",
    "        continue  # drop this label entirely\n",
    "    if n > MAX_SPECTRA_PER_LABEL:\n",
    "        capped_too_many += 1\n",
    "        chosen = np.random.choice(idxs, size=MAX_SPECTRA_PER_LABEL, replace=False)\n",
    "        keep_indices.extend(chosen.tolist())\n",
    "    else:\n",
    "        keep_indices.extend(idxs)\n",
    "\n",
    "keep_indices = np.array(keep_indices, dtype=int)\n",
    "\n",
    "print(f\"Labels dropped for having < {MIN_SPECTRA_PER_LABEL} spectra: {dropped_too_few}\")\n",
    "print(f\"Labels capped at {MAX_SPECTRA_PER_LABEL} spectra: {capped_too_many}\")\n",
    "print(\"Total spectra after per-label MIN/MAX filter:\", len(keep_indices))\n",
    "\n",
    "if len(keep_indices) == 0:\n",
    "    raise ValueError(\n",
    "        \"No spectra left after applying MIN_SPECTRA_PER_LABEL and MAX_SPECTRA_PER_LABEL. \"\n",
    "        \"Relax thresholds or check dataset.\"\n",
    "    )\n",
    "\n",
    "# Apply per-label filter\n",
    "ms2_lib   = ms2_lib[keep_indices]\n",
    "label_str = label_str[keep_indices]\n",
    "\n",
    "N_total, n_bins = ms2_lib.shape\n",
    "print(f\"Spectra shape after per-label MIN/MAX: (N, n_bins) = {ms2_lib.shape}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. Restrict to chosen LABEL rank range (frequency-based)\n",
    "# =========================\n",
    "all_unique_lbl, all_lbl_ids = np.unique(label_str, return_inverse=True)\n",
    "counts = np.bincount(all_lbl_ids)\n",
    "print(\"Total unique labels before rank-frequency filter:\", len(all_unique_lbl))\n",
    "\n",
    "# Sort labels by frequency (descending)\n",
    "sorted_class_indices = np.argsort(-counts)  # 0..(n_classes-1), high→low frequency\n",
    "n_classes_total = len(sorted_class_indices)\n",
    "\n",
    "# Clip requested rank range to available classes\n",
    "start_rank = max(1, START_RANK)\n",
    "end_rank   = min(END_RANK, n_classes_total)\n",
    "\n",
    "if end_rank < start_rank:\n",
    "    raise ValueError(\n",
    "        f\"No valid LABEL ranks after clipping to available classes. \"\n",
    "        f\"Requested {START_RANK}-{END_RANK}, available 1-{n_classes_total}.\"\n",
    "    )\n",
    "\n",
    "start_idx = start_rank - 1  # inclusive\n",
    "end_idx   = end_rank        # exclusive in slicing\n",
    "\n",
    "block_indices = sorted_class_indices[start_idx:end_idx]\n",
    "print(f\"\\nUsing LABEL ranks {start_rank}..{end_rank} (1-based).\")\n",
    "print(f\"Number of LABEL classes in this block: {len(block_indices)}\")\n",
    "\n",
    "if len(block_indices) == 0:\n",
    "    raise ValueError(\n",
    "        \"No LABEL classes in the requested rank range. \"\n",
    "        \"Try a different range (e.g. 1..50).\"\n",
    "    )\n",
    "\n",
    "block_set = set(block_indices.tolist())\n",
    "\n",
    "mask_block        = np.isin(all_lbl_ids, list(block_set))\n",
    "label_str_block   = label_str[mask_block]\n",
    "ms2_lib_block     = ms2_lib[mask_block]\n",
    "\n",
    "print(\"Spectra after LABEL-block (rank) filter:\", len(label_str_block))\n",
    "\n",
    "# Re-encode LABELs compactly 0..(K-1)\n",
    "unique_lbl_block, lbl_ids_block = np.unique(label_str_block, return_inverse=True)\n",
    "unique_lbl_block = np.array(unique_lbl_block, dtype=object)\n",
    "n_classes = len(unique_lbl_block)\n",
    "print(\"Number of unique LABELs in block:\", n_classes)\n",
    "\n",
    "N, n_bins = ms2_lib_block.shape\n",
    "print(f\"Final spectra shape after LABEL block filter: (N, n_bins) = {ms2_lib_block.shape}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. Use ALL filtered spectra as training data (NO test split)\n",
    "# =========================\n",
    "train_ms2   = ms2_lib_block\n",
    "train_lblid = lbl_ids_block\n",
    "\n",
    "print(f\"\\nTraining on ALL spectra after filtering: {len(train_ms2)}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6. Dataset & balanced sampler\n",
    "# =========================\n",
    "class SpectraDataset(Dataset):\n",
    "    def __init__(self, ms2, labels):\n",
    "        self.ms2    = torch.tensor(ms2,   dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ms2.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"spectrum\": self.ms2[idx],\n",
    "            \"label\":    self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "class ClassBalancedBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Samples batches with:\n",
    "      - classes_per_batch distinct labels\n",
    "      - samples_per_class spectra for each label\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, classes_per_batch=32, samples_per_class=4):\n",
    "        self.labels = np.array(labels, dtype=np.int64)\n",
    "        self.classes_per_batch = classes_per_batch\n",
    "        self.samples_per_class = samples_per_class\n",
    "\n",
    "        self.class_to_indices = defaultdict(list)\n",
    "        for i, lab in enumerate(self.labels):\n",
    "            self.class_to_indices[lab].append(i)\n",
    "\n",
    "        self.unique_classes = np.array(list(self.class_to_indices.keys()))\n",
    "        self.batch_size = self.classes_per_batch * self.samples_per_class\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if len(self.unique_classes) <= self.classes_per_batch:\n",
    "                chosen_classes = self.unique_classes\n",
    "            else:\n",
    "                chosen_classes = np.random.choice(\n",
    "                    self.unique_classes,\n",
    "                    size=self.classes_per_batch,\n",
    "                    replace=False\n",
    "                )\n",
    "\n",
    "            batch_indices = []\n",
    "            for c in chosen_classes:\n",
    "                idxs = self.class_to_indices[c]\n",
    "                if len(idxs) >= self.samples_per_class:\n",
    "                    chosen = np.random.choice(idxs, size=self.samples_per_class, replace=False)\n",
    "                else:\n",
    "                    chosen = np.random.choice(idxs, size=self.samples_per_class, replace=True)\n",
    "                batch_indices.extend(chosen.tolist())\n",
    "            yield batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        # approximate number of batches per epoch\n",
    "        return max(1, len(self.labels) // self.batch_size)\n",
    "\n",
    "\n",
    "train_dataset = SpectraDataset(train_ms2, train_lblid)\n",
    "train_sampler = ClassBalancedBatchSampler(\n",
    "    labels=train_lblid,\n",
    "    classes_per_batch=BATCH_P_CLASSES,\n",
    "    samples_per_class=BATCH_K_SPECTRA\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_sampler\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7. Model: Spectrum Encoder\n",
    "# =========================\n",
    "class SpectrumEncoder(nn.Module):\n",
    "    def __init__(self, n_bins, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_bins, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        if torch.isnan(z).any():\n",
    "            print(\"Warning: NaN detected in encoder output, applying nan_to_num.\")\n",
    "            z = torch.nan_to_num(z, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8. Safe supervised contrastive loss\n",
    "# =========================\n",
    "def supervised_contrastive_loss_safe(emb, labels, temperature=0.07):\n",
    "    \"\"\"\n",
    "    emb    : (B, D) L2-normalized embeddings\n",
    "    labels : (B,) int label ids\n",
    "    \"\"\"\n",
    "    device = emb.device\n",
    "    B, D = emb.shape\n",
    "    if B <= 1:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    # Cosine similarity\n",
    "    sim = emb @ emb.t()\n",
    "    sim = torch.clamp(sim / temperature, min=-50.0, max=50.0)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    valid_anchors = 0\n",
    "\n",
    "    for i in range(B):\n",
    "        sim_i = sim[i].clone()\n",
    "        sim_i[i] = float(\"-inf\")\n",
    "\n",
    "        pos_mask_i = (labels == labels[i]) & (torch.arange(B, device=device) != i)\n",
    "        if not pos_mask_i.any():\n",
    "            continue\n",
    "\n",
    "        finite_mask = torch.isfinite(sim_i)\n",
    "        if not finite_mask.any():\n",
    "            continue\n",
    "\n",
    "        max_sim = sim_i[finite_mask].max()\n",
    "        shifted = sim_i - max_sim\n",
    "        shifted[~finite_mask] = float(\"-inf\")\n",
    "\n",
    "        exp_all = torch.exp(shifted)\n",
    "        denom = exp_all.sum()\n",
    "        if not torch.isfinite(denom) or denom <= 0:\n",
    "            continue\n",
    "\n",
    "        exp_pos = exp_all * pos_mask_i.float()\n",
    "        num = exp_pos.sum()\n",
    "        if not torch.isfinite(num) or num <= 0:\n",
    "            continue\n",
    "\n",
    "        loss_i = -torch.log(num / denom)\n",
    "        if not torch.isfinite(loss_i):\n",
    "            continue\n",
    "\n",
    "        total_loss += loss_i\n",
    "        valid_anchors += 1\n",
    "\n",
    "    if valid_anchors == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = total_loss / valid_anchors\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9. Training loop (contrastive spectrum encoder)\n",
    "# =========================\n",
    "model = SpectrumEncoder(n_bins=n_bins, emb_dim=EMB_DIM).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "print(\"\\nsteps_per_epoch:\", steps_per_epoch)\n",
    "\n",
    "# Sanity check\n",
    "print(\"Running a sanity check on one batch...\")\n",
    "batch_example = next(iter(train_loader))\n",
    "x_ex = batch_example[\"spectrum\"].to(DEVICE)\n",
    "y_ex = batch_example[\"label\"].to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    emb_ex = model(x_ex)\n",
    "    print(\"Any NaN in emb_ex?\", torch.isnan(emb_ex).any().item())\n",
    "    loss_ex = supervised_contrastive_loss_safe(emb_ex, y_ex, temperature=TEMPERATURE)\n",
    "    print(\"Initial loss (sanity check):\", loss_ex.item())\n",
    "\n",
    "print(\"\\nStarting training (contrastive spectrum encoder)...\")\n",
    "spectrum_epoch_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    effective_steps = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        if step >= steps_per_epoch:\n",
    "            break  # prevent infinite epoch\n",
    "\n",
    "        x = batch[\"spectrum\"].to(DEVICE)\n",
    "        y = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        emb = model(x)\n",
    "        loss = supervised_contrastive_loss_safe(emb, y, temperature=TEMPERATURE)\n",
    "\n",
    "        if torch.isnan(loss) or not torch.isfinite(loss):\n",
    "            print(f\"NaN/inf loss at epoch {epoch+1}, step {step+1}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        effective_steps += 1\n",
    "\n",
    "    avg_loss = running_loss / max(1, effective_steps)\n",
    "    spectrum_epoch_losses.append(avg_loss)\n",
    "    print(f\"[Spectrum] Epoch {epoch+1}/{EPOCHS}  Avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Contrastive training done.\")\n",
    "print(\"Spectrum encoder losses per epoch:\", spectrum_epoch_losses)\n",
    "\n",
    "# Save spectrum loss curve as CSV\n",
    "spectrum_loss_df = pd.DataFrame({\n",
    "    \"epoch\": np.arange(1, EPOCHS + 1),\n",
    "    \"loss\": spectrum_epoch_losses\n",
    "})\n",
    "spectrum_loss_csv_path = os.path.join(OUTPUT_DIR, \"spectrum_epoch_losses.csv\")\n",
    "spectrum_loss_df.to_csv(spectrum_loss_csv_path, index=False)\n",
    "print(f\"Spectrum epoch losses saved to {spectrum_loss_csv_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 10. Build train embedding index\n",
    "# =========================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_emb = []\n",
    "    batch_size_eval = 2048\n",
    "    for i in range(0, len(train_ms2), batch_size_eval):\n",
    "        batch = torch.tensor(\n",
    "            train_ms2[i:i+batch_size_eval],\n",
    "            dtype=torch.float32,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        z = model(batch)\n",
    "        train_emb.append(z.cpu().numpy())\n",
    "    train_emb = np.concatenate(train_emb, axis=0)\n",
    "\n",
    "print(\"\\nTrain embeddings shape:\", train_emb.shape)\n",
    "train_labels = train_lblid\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 11. Save spectrum encoder model\n",
    "# =========================\n",
    "MODEL_PATH = os.path.join(\n",
    "    OUTPUT_DIR,\n",
    "    f\"contrastive_spectrum_encoder_Accession_\"\n",
    "    f\"min{MIN_SPECTRA_PER_LABEL}_max{MAX_SPECTRA_PER_LABEL}_\"\n",
    "    f\"ranks_{start_rank}_{end_rank}.pth\"\n",
    ")\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"emb_dim\": EMB_DIM,\n",
    "    \"n_bins\": n_bins,\n",
    "    \"unique_label_block\": list(unique_lbl_block),  # accession labels for this block\n",
    "    \"rank_start\": start_rank,\n",
    "    \"rank_end\": end_rank,\n",
    "    \"min_spectra_per_label\": MIN_SPECTRA_PER_LABEL,\n",
    "    \"max_spectra_per_label\": MAX_SPECTRA_PER_LABEL,\n",
    "}, MODEL_PATH)\n",
    "\n",
    "print(f\"Contrastive spectrum model saved to {MODEL_PATH}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 12. Save spectrum embedding index for retrieval\n",
    "# =========================\n",
    "acc_per_spectrum = np.array(\n",
    "    [str(unique_lbl_block[label]) for label in train_labels],\n",
    "    dtype=object\n",
    ")\n",
    "\n",
    "INDEX_PATH = os.path.join(OUTPUT_DIR, \"train_spectrum_embeddings.npz\")\n",
    "np.savez(\n",
    "    INDEX_PATH,\n",
    "    embeddings=train_emb,         # (N, 64)\n",
    "    labels=train_labels,          # (N,)\n",
    "    accessions=acc_per_spectrum   # (N,)\n",
    ")\n",
    "print(f\"Spectrum embedding index saved to {INDEX_PATH}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 13. Load Accession → AA sequence from FASTA\n",
    "# =========================\n",
    "print(\"\\nParsing FASTA:\", FASTA_PATH)\n",
    "acc_to_seq = parse_fasta_to_dict(FASTA_PATH)\n",
    "print(f\"Found {len(acc_to_seq)} accessions with sequences in FASTA.\")\n",
    "\n",
    "# =========================\n",
    "# 14. Build per-Accession target embeddings for PLM\n",
    "# =========================\n",
    "acc_block = np.array(label_str_block, dtype=object)   # acc per spectrum row in block\n",
    "\n",
    "n_labels_plm = n_classes  # same as number of unique_lbl_block\n",
    "\n",
    "label_ids = []\n",
    "target_embs = []\n",
    "seqs_for_labels = []\n",
    "\n",
    "missing_seq = 0\n",
    "\n",
    "for lab_id in range(n_labels_plm):\n",
    "    acc = str(unique_lbl_block[lab_id])\n",
    "\n",
    "    # indices of spectra belonging to this label\n",
    "    idxs = np.where(train_labels == lab_id)[0]\n",
    "    if len(idxs) == 0:\n",
    "        continue\n",
    "\n",
    "    # mean spectrum embedding for this Accession (target for regression)\n",
    "    mean_emb = train_emb[idxs].mean(axis=0)\n",
    "\n",
    "    # look up AA sequence from FASTA\n",
    "    if acc not in acc_to_seq:\n",
    "        missing_seq += 1\n",
    "        continue\n",
    "\n",
    "    seq_rep = acc_to_seq[acc]  # AA sequence string\n",
    "\n",
    "    label_ids.append(lab_id)\n",
    "    target_embs.append(mean_emb)\n",
    "    seqs_for_labels.append(seq_rep)\n",
    "\n",
    "label_ids = np.array(label_ids, dtype=int)\n",
    "target_embs = np.stack(target_embs, axis=0).astype(np.float32)\n",
    "\n",
    "print(f\"\\nPer-Accession targets for PLM:\")\n",
    "print(\"  #labels with both AA sequence and embedding:\", len(label_ids))\n",
    "print(\"  #labels missing sequence in FASTA:\", missing_seq)\n",
    "print(\"  target_embs shape:\", target_embs.shape)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 15. Protein LM → MS2 embedding regressor\n",
    "# =========================\n",
    "print(\"\\nLoading protein language model:\", PLM_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PLM_NAME)\n",
    "backbone = AutoModel.from_pretrained(PLM_NAME)\n",
    "\n",
    "\n",
    "class ProteinToSpectrumRegressor(nn.Module):\n",
    "    def __init__(self, backbone, emb_dim, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        hidden_size = backbone.config.hidden_size\n",
    "        self.reg_head = nn.Linear(hidden_size, emb_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        token_embs = outputs.last_hidden_state   # (B, L, H)\n",
    "        cls = token_embs[:, 0, :]               # (B, H)\n",
    "        z = self.reg_head(cls)                  # (B, emb_dim)\n",
    "        z = F.normalize(z, dim=-1)              # keep in same manifold as train_emb\n",
    "        return z\n",
    "\n",
    "\n",
    "class ProteinSeqDataset(Dataset):\n",
    "    def __init__(self, sequences, targets, tokenizer, max_len=1024):\n",
    "        self.sequences = list(sequences)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]      # AA sequence\n",
    "        tgt = self.targets[idx]\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            seq,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"target\": tgt\n",
    "        }\n",
    "\n",
    "\n",
    "def cosine_regression_loss(pred, target):\n",
    "    \"\"\"\n",
    "    Encourage high cosine similarity between predicted and target embeddings.\n",
    "    pred, target: (B, D)\n",
    "    \"\"\"\n",
    "    pred = F.normalize(pred, dim=-1)\n",
    "    target = F.normalize(target, dim=-1)\n",
    "    cos = (pred * target).sum(dim=-1)\n",
    "    return 1.0 - cos.mean()\n",
    "\n",
    "\n",
    "plm_model = ProteinToSpectrumRegressor(\n",
    "    backbone,\n",
    "    emb_dim=EMB_DIM,\n",
    "    freeze_backbone=FREEZE_BACKBONE\n",
    ").to(DEVICE)\n",
    "\n",
    "plm_dataset = ProteinSeqDataset(\n",
    "    sequences=seqs_for_labels,\n",
    "    targets=target_embs,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_SEQ_LEN\n",
    ")\n",
    "\n",
    "plm_loader = DataLoader(\n",
    "    plm_dataset,\n",
    "    batch_size=PLM_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 16. Train PLM → MS2 regressor\n",
    "# =========================\n",
    "optimizer_plm = torch.optim.AdamW(plm_model.parameters(), lr=PLM_LR)\n",
    "\n",
    "print(\"\\nStarting PLM fine-tuning to predict MS2 embeddings...\")\n",
    "plm_epoch_losses = []\n",
    "for epoch in range(PLM_EPOCHS):\n",
    "    plm_model.train()\n",
    "    running_loss = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    for batch in plm_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        target = batch[\"target\"].to(DEVICE)\n",
    "\n",
    "        pred = plm_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = cosine_regression_loss(pred, target)\n",
    "\n",
    "        optimizer_plm.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_plm.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_steps += 1\n",
    "\n",
    "    avg_loss = running_loss / max(1, n_steps)\n",
    "    plm_epoch_losses.append(avg_loss)\n",
    "    print(f\"[PLM] Epoch {epoch+1}/{PLM_EPOCHS}  Avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"PLM fine-tuning done.\")\n",
    "print(\"PLM regressor losses per epoch:\", plm_epoch_losses)\n",
    "\n",
    "# Save PLM loss curve as CSV\n",
    "plm_loss_df = pd.DataFrame({\n",
    "    \"epoch\": np.arange(1, PLM_EPOCHS + 1),\n",
    "    \"loss\": plm_epoch_losses\n",
    "})\n",
    "plm_loss_csv_path = os.path.join(OUTPUT_DIR, \"plm_epoch_losses.csv\")\n",
    "plm_loss_df.to_csv(plm_loss_csv_path, index=False)\n",
    "print(f\"PLM epoch losses saved to {plm_loss_csv_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 17. Save Protein→MS2 embedding model\n",
    "# =========================\n",
    "PLM_MODEL_PATH = os.path.join(\n",
    "    OUTPUT_DIR,\n",
    "    f\"protein_to_ms2_plm_{PLM_NAME.replace('/', '_')}_\"\n",
    "    f\"emb{EMB_DIM}_ranks_{start_rank}_{end_rank}.pth\"\n",
    ")\n",
    "\n",
    "torch.save({\n",
    "    \"plm_name\": PLM_NAME,\n",
    "    \"model_state_dict\": plm_model.state_dict(),\n",
    "    \"emb_dim\": EMB_DIM,\n",
    "    \"rank_start\": start_rank,\n",
    "    \"rank_end\": end_rank,\n",
    "    \"label_ids\": label_ids.tolist(),\n",
    "    \"accessions\": [str(unique_lbl_block[i]) for i in label_ids],\n",
    "}, PLM_MODEL_PATH)\n",
    "\n",
    "print(f\"Protein→MS2 embedding model saved to {PLM_MODEL_PATH}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 18. Function: embed a sequence with trained PLM\n",
    "# =========================\n",
    "def embed_sequence(seq: str):\n",
    "    enc = tokenizer(\n",
    "        seq,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_SEQ_LEN\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    plm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = plm_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    return z[0].cpu().numpy()   # (EMB_DIM,)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 19. Quick sanity check on a test sequence\n",
    "# =========================\n",
    "test_seq = (\n",
    "    \"MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDRFKHLKTE\"\n",
    "    \"AEMKASEDLKKHGATVLTALGGILKKKGKH\"\n",
    ")\n",
    "\n",
    "z_seq = embed_sequence(test_seq)\n",
    "print(\"\\nTest sequence embedding shape:\", z_seq.shape)\n",
    "print(\"First 10 dims:\", z_seq[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
