{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c34cde5",
   "metadata": {},
   "source": [
    "Combine multiple tdportal reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72219622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined 6 files into F:\\20251115\\tdportal\\combined.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# --- Folder path containing your CSV files ---\n",
    "folder = r\"F:\\20251115\\tdportal\"   # ⬅️ change this to your actual folder path\n",
    "\n",
    "# --- Find all CSV files in the folder ---\n",
    "csv_files = glob.glob(os.path.join(folder, \"*.csv\"))\n",
    "\n",
    "# --- Read each CSV and combine them by row ---\n",
    "dfs = [pd.read_csv(f) for f in csv_files]\n",
    "combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# --- Save the combined CSV ---\n",
    "output_path = os.path.join(folder, \"combined.csv\")\n",
    "combined.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Combined {len(csv_files)} files into {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c1214",
   "metadata": {},
   "source": [
    "Combine multiple h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Combine multiple HDF5 files with identical structure into a single compressed HDF5.\n",
    "\n",
    "Expected datasets in each input file:\n",
    "  - file_name      (N,)        |S59\n",
    "  - group_name     (N,)        |S6\n",
    "  - ms2_lib        (N, 1600)   float16\n",
    "  - precursor_mz   (N,)        float32\n",
    "  - rt_min         (N,)        float32\n",
    "  - scan           (N,)        int32\n",
    "\n",
    "Output:\n",
    "  - One HDF5 with the same dataset names, concatenated along axis 0,\n",
    "    stored with gzip compression + shuffle + chunking (npz-like behavior).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# User parameters\n",
    "# -----------------------------\n",
    "INPUT_DIR = r\"F:\\20251115\\spectra_h5\"      # <-- change this\n",
    "OUTPUT_FILE = r\"F:\\20251115\\spectra_h5/combined.h5\"    # <-- change this\n",
    "PATTERN = \"*.h5\"                         # which input files to combine\n",
    "\n",
    "# I/O chunk size when reading/copying (None = whole file at once; use e.g. 100_000 for huge datasets)\n",
    "COPY_CHUNK_ROWS = 100_000\n",
    "\n",
    "# HDF5 dataset compression / chunking\n",
    "H5_COMPRESSION = \"gzip\"     # \"gzip\" is closest to npz; you can use \"lzf\" for faster but less tight\n",
    "H5_COMP_OPTS = 4            # 1–9 for gzip, higher = more compression, slower\n",
    "H5_ROW_CHUNK = 4096         # chunk length along the row axis for HDF5 datasets\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def get_files(input_dir, pattern=\"*.h5\"):\n",
    "    files = sorted(glob.glob(os.path.join(input_dir, pattern)))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files found in {input_dir!r} matching {pattern!r}\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def inspect_first_file(example_file):\n",
    "    \"\"\"\n",
    "    Read the first file and capture shapes/dtypes to define the output file.\n",
    "    Also verifies that required datasets exist.\n",
    "    \"\"\"\n",
    "    expected_keys = [\"file_name\", \"group_name\", \"ms2_lib\",\n",
    "                     \"precursor_mz\", \"rt_min\", \"scan\"]\n",
    "    info = {}\n",
    "\n",
    "    with h5py.File(example_file, \"r\") as f:\n",
    "        for k in expected_keys:\n",
    "            if k not in f:\n",
    "                raise KeyError(f\"Dataset {k!r} not found in {example_file}\")\n",
    "            ds = f[k]\n",
    "            info[k] = {\n",
    "                \"shape\": ds.shape,\n",
    "                \"dtype\": ds.dtype,\n",
    "            }\n",
    "\n",
    "        # quick consistency checks\n",
    "        n = info[\"file_name\"][\"shape\"][0]\n",
    "        for k in expected_keys:\n",
    "            if f[k].shape[0] != n:\n",
    "                raise ValueError(\n",
    "                    f\"Dataset {k!r} in {example_file!r} has first dim \"\n",
    "                    f\"{f[k].shape[0]} != file_name first dim {n}\"\n",
    "                )\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def compute_total_rows(files):\n",
    "    \"\"\"\n",
    "    Sum up rows across all files and sanity-check that\n",
    "    each file has internally consistent first dimensions.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for fp in files:\n",
    "        with h5py.File(fp, \"r\") as f:\n",
    "            n = f[\"file_name\"].shape[0]\n",
    "            assert f[\"group_name\"].shape[0] == n\n",
    "            assert f[\"ms2_lib\"].shape[0] == n\n",
    "            assert f[\"precursor_mz\"].shape[0] == n\n",
    "            assert f[\"rt_min\"].shape[0] == n\n",
    "            assert f[\"scan\"].shape[0] == n\n",
    "        total += n\n",
    "    return total\n",
    "\n",
    "\n",
    "def create_output_file(output_file, first_info, total_rows,\n",
    "                       compression=H5_COMPRESSION,\n",
    "                       compression_opts=H5_COMP_OPTS,\n",
    "                       row_chunk=H5_ROW_CHUNK):\n",
    "    \"\"\"\n",
    "    Create the output HDF5 with compressed, chunked datasets.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "\n",
    "    ms2_dim = first_info[\"ms2_lib\"][\"shape\"][1]\n",
    "\n",
    "    f_out = h5py.File(output_file, \"w\")\n",
    "\n",
    "    # Chunk shapes\n",
    "    chunk_scalar = (row_chunk,)\n",
    "    chunk_ms2 = (row_chunk, ms2_dim)\n",
    "\n",
    "    d_file_name = f_out.create_dataset(\n",
    "        \"file_name\",\n",
    "        shape=(total_rows,),\n",
    "        dtype=first_info[\"file_name\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_scalar,\n",
    "    )\n",
    "\n",
    "    d_group_name = f_out.create_dataset(\n",
    "        \"group_name\",\n",
    "        shape=(total_rows,),\n",
    "        dtype=first_info[\"group_name\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_scalar,\n",
    "    )\n",
    "\n",
    "    d_ms2_lib = f_out.create_dataset(\n",
    "        \"ms2_lib\",\n",
    "        shape=(total_rows, ms2_dim),\n",
    "        dtype=first_info[\"ms2_lib\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_ms2,\n",
    "    )\n",
    "\n",
    "    d_precursor_mz = f_out.create_dataset(\n",
    "        \"precursor_mz\",\n",
    "        shape=(total_rows,),\n",
    "        dtype=first_info[\"precursor_mz\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_scalar,\n",
    "    )\n",
    "\n",
    "    d_rt_min = f_out.create_dataset(\n",
    "        \"rt_min\",\n",
    "        shape=(total_rows,),\n",
    "        dtype=first_info[\"rt_min\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_scalar,\n",
    "    )\n",
    "\n",
    "    d_scan = f_out.create_dataset(\n",
    "        \"scan\",\n",
    "        shape=(total_rows,),\n",
    "        dtype=first_info[\"scan\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_scalar,\n",
    "    )\n",
    "\n",
    "    dsets_out = {\n",
    "        \"file_name\": d_file_name,\n",
    "        \"group_name\": d_group_name,\n",
    "        \"ms2_lib\": d_ms2_lib,\n",
    "        \"precursor_mz\": d_precursor_mz,\n",
    "        \"rt_min\": d_rt_min,\n",
    "        \"scan\": d_scan,\n",
    "    }\n",
    "\n",
    "    return f_out, dsets_out\n",
    "\n",
    "\n",
    "def copy_data(files, dsets_out, chunk_rows=None):\n",
    "    \"\"\"\n",
    "    Concatenate datasets from all input files into output datasets along axis 0.\n",
    "\n",
    "    If chunk_rows is None:\n",
    "        - copy each file in one go (simple, more RAM).\n",
    "    Else:\n",
    "        - copy in slices of size chunk_rows (safer for huge files).\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "\n",
    "    for fp in files:\n",
    "        with h5py.File(fp, \"r\") as f_in:\n",
    "            n_rows = f_in[\"file_name\"].shape[0]\n",
    "            print(f\"Processing {fp} with {n_rows} rows (writing from offset {offset})\")\n",
    "\n",
    "            if chunk_rows is None:\n",
    "                # Whole file at once\n",
    "                sl = slice(offset, offset + n_rows)\n",
    "                dsets_out[\"file_name\"][sl] = f_in[\"file_name\"][...]\n",
    "                dsets_out[\"group_name\"][sl] = f_in[\"group_name\"][...]\n",
    "                dsets_out[\"ms2_lib\"][sl] = f_in[\"ms2_lib\"][...]\n",
    "                dsets_out[\"precursor_mz\"][sl] = f_in[\"precursor_mz\"][...]\n",
    "                dsets_out[\"rt_min\"][sl] = f_in[\"rt_min\"][...]\n",
    "                dsets_out[\"scan\"][sl] = f_in[\"scan\"][...]\n",
    "            else:\n",
    "                # Chunked copy\n",
    "                for start in range(0, n_rows, chunk_rows):\n",
    "                    end = min(start + chunk_rows, n_rows)\n",
    "                    local_sl = slice(start, end)\n",
    "                    global_sl = slice(offset + start, offset + end)\n",
    "\n",
    "                    dsets_out[\"file_name\"][global_sl] = f_in[\"file_name\"][local_sl]\n",
    "                    dsets_out[\"group_name\"][global_sl] = f_in[\"group_name\"][local_sl]\n",
    "                    dsets_out[\"ms2_lib\"][global_sl] = f_in[\"ms2_lib\"][local_sl]\n",
    "                    dsets_out[\"precursor_mz\"][global_sl] = f_in[\"precursor_mz\"][local_sl]\n",
    "                    dsets_out[\"rt_min\"][global_sl] = f_in[\"rt_min\"][local_sl]\n",
    "                    dsets_out[\"scan\"][global_sl] = f_in[\"scan\"][local_sl]\n",
    "\n",
    "        offset += n_rows\n",
    "\n",
    "    print(f\"Done. Total rows written: {offset}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    files = get_files(INPUT_DIR, PATTERN)\n",
    "    print(f\"Found {len(files)} input files:\")\n",
    "    for f in files:\n",
    "        print(\"  -\", f)\n",
    "\n",
    "    first_info = inspect_first_file(files[0])\n",
    "    total_rows = compute_total_rows(files)\n",
    "    print(f\"Total rows across all files: {total_rows}\")\n",
    "\n",
    "    f_out, dsets_out = create_output_file(OUTPUT_FILE, first_info, total_rows)\n",
    "    print(f\"Created output file: {OUTPUT_FILE}\")\n",
    "\n",
    "    try:\n",
    "        copy_data(files, dsets_out, chunk_rows=COPY_CHUNK_ROWS)\n",
    "        print(\"All data copied successfully.\")\n",
    "    finally:\n",
    "        f_out.close()\n",
    "        print(\"Output file closed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1003bcf",
   "metadata": {},
   "source": [
    "ID import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7203614c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_2860\\3293070920.py:25: DtypeWarning: Columns (9,10,25,26,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tdportal = pd.read_csv(tdportal_csv)\n",
      "Matching scans: 100%|██████████| 2202567/2202567 [00:55<00:00, 39615.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MATCHING SUMMARY =====\n",
      "Total samples in H5: 352\n",
      "Samples in tdportal: 352\n",
      "Matched samples:     350\n",
      "Unmatched samples:   352\n",
      "----------------------------------\n",
      "Total scans in H5:   2202567\n",
      "Matched scans:       398060\n",
      "Unmatched scans:     1804507\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def annotate_h5_with_tdportal(tdportal_csv, in_h5_path, out_h5_path):\n",
    "    \"\"\"\n",
    "    Read tdportal CSV and an input H5 databank, match scans to IDs, and\n",
    "    write a new H5 file with added annotation datasets:\n",
    "        - sequence\n",
    "        - MASS\n",
    "        - Accession\n",
    "        - PFR\n",
    "    \n",
    "    ALSO PRINTS:\n",
    "        - number of samples (file names)\n",
    "        - matched samples vs unmatched samples\n",
    "        - total scans matched vs unmatched\n",
    "    \"\"\"\n",
    "    # ------------------------------\n",
    "    # 1) Load tdportal\n",
    "    # ------------------------------\n",
    "    tdportal = pd.read_csv(tdportal_csv)\n",
    "\n",
    "    def str_to_int_list(s):\n",
    "        \"\"\"Extract all integers from a string as a list.\"\"\"\n",
    "        return [int(x) for x in re.findall(r'\\d+', str(s))]\n",
    "\n",
    "    # Build mapping: (sample_name, scan) -> tdportal row index\n",
    "    mapping = {}\n",
    "    samples_in_tdportal = set()\n",
    "\n",
    "    for i, row in tdportal.iterrows():\n",
    "        sample = row['File Name']\n",
    "        samples_in_tdportal.add(sample)\n",
    "        scans = str_to_int_list(row['Fragment Scans'])\n",
    "        for sc in scans:\n",
    "            mapping[(sample, sc)] = i\n",
    "\n",
    "    # ------------------------------\n",
    "    # 2) Load input H5 databank\n",
    "    # ------------------------------\n",
    "    with h5py.File(in_h5_path, \"r\") as f:\n",
    "        file_name = f[\"file_name\"][()]  # (N,)\n",
    "        scan      = f[\"scan\"][()]      # (N,)\n",
    "\n",
    "        # decode bytes to str if needed\n",
    "        def maybe_decode(arr):\n",
    "            if isinstance(arr[0], (bytes, np.bytes_)):\n",
    "                return np.array([x.decode(\"utf-8\") for x in arr], dtype=object)\n",
    "            return arr\n",
    "\n",
    "        file_name = maybe_decode(file_name)\n",
    "        scan = scan.astype(int)\n",
    "\n",
    "        N = len(scan)\n",
    "\n",
    "        # Pre-allocate outputs\n",
    "        sequence  = [\"\"] * N\n",
    "        MASS      = [np.nan] * N\n",
    "        Accession = [\"\"] * N\n",
    "        PFR       = [np.nan] * N\n",
    "\n",
    "        # Statistics tracking\n",
    "        matched_scan_count = 0\n",
    "        unmatched_scan_count = 0\n",
    "        matched_samples = set()\n",
    "        unmatched_samples = set()\n",
    "\n",
    "        for i in trange(N, desc=\"Matching scans\"):\n",
    "            sname = file_name[i]\n",
    "            sc    = int(scan[i])\n",
    "            key   = (sname, sc)\n",
    "\n",
    "            if key in mapping:\n",
    "                matched_scan_count += 1\n",
    "                matched_samples.add(sname)\n",
    "\n",
    "                idx = mapping[key]\n",
    "                row = tdportal.loc[idx]\n",
    "                sequence[i]  = str(row.get('Sequence', \"\"))\n",
    "                MASS[i]      = float(row.get('Average Mass', np.nan))\n",
    "                Accession[i] = str(row.get('Accession', \"\"))\n",
    "                PFR[i]       = float(row.get('PFR', np.nan))\n",
    "\n",
    "            else:\n",
    "                unmatched_scan_count += 1\n",
    "                unmatched_samples.add(sname)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 3) Print statistics\n",
    "    # ------------------------------\n",
    "    total_samples = len(np.unique(file_name))\n",
    "\n",
    "    print(\"\\n===== MATCHING SUMMARY =====\")\n",
    "    print(f\"Total samples in H5: {total_samples}\")\n",
    "    print(f\"Samples in tdportal: {len(samples_in_tdportal)}\")\n",
    "    print(f\"Matched samples:     {len(matched_samples)}\")\n",
    "    print(f\"Unmatched samples:   {len(unmatched_samples)}\")\n",
    "\n",
    "    print(\"----------------------------------\")\n",
    "    print(f\"Total scans in H5:   {N}\")\n",
    "    print(f\"Matched scans:       {matched_scan_count}\")\n",
    "    print(f\"Unmatched scans:     {unmatched_scan_count}\")\n",
    "    print(\"==================================\\n\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # 4) Write new H5 with annotations\n",
    "    # ------------------------------\n",
    "    dt_str = h5py.string_dtype(encoding='utf-8')\n",
    "\n",
    "    with h5py.File(in_h5_path, \"r\") as fin, h5py.File(out_h5_path, \"w\") as fout:\n",
    "        # Copy datasets\n",
    "        for name, item in fin.items():\n",
    "            fout.create_dataset(name, data=item[()], compression=\"gzip\")\n",
    "\n",
    "        # Add annotation datasets\n",
    "        fout.create_dataset(\"sequence\",  data=np.array(sequence,  dtype=dt_str))\n",
    "        fout.create_dataset(\"MASS\",      data=np.array(MASS,      dtype=np.float32))\n",
    "        fout.create_dataset(\"Accession\", data=np.array(Accession, dtype=dt_str))\n",
    "        fout.create_dataset(\"PFR\",       data=np.array(PFR,       dtype=np.float32))\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Example usage\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    tdportal_csv = r\"F:\\20251115\\tdportal\\combined.csv\"\n",
    "    in_h5_path   = r\"F:\\20251115\\spectra_h5\\combined.h5\"\n",
    "    out_h5_path  = r\"F:\\20251115\\spectra_h5\\combined_annotated.h5\"\n",
    "\n",
    "    annotate_h5_with_tdportal(tdportal_csv, in_h5_path, out_h5_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
