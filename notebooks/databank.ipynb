{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad71ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in file:\n",
      "   file_name\n",
      "   group_name\n",
      "   ms2_lib\n",
      "   precursor_mz\n",
      "   rt_min\n",
      "   scan\n",
      "\n",
      "Full structure:\n",
      "[DATASET] file_name - shape: (983170,), dtype: |S59\n",
      "[DATASET] group_name - shape: (983170,), dtype: |S6\n",
      "[DATASET] ms2_lib - shape: (983170, 1600), dtype: float16\n",
      "[DATASET] precursor_mz - shape: (983170,), dtype: float32\n",
      "[DATASET] rt_min - shape: (983170,), dtype: float32\n",
      "[DATASET] scan - shape: (983170,), dtype: int32\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# --- path to your file ---\n",
    "filepath = \"F:/20251115/ms2_tissue_dataset.h5\"\n",
    "\n",
    "with h5py.File(filepath, \"r\") as f:\n",
    "    # List top-level groups / datasets\n",
    "    print(\"Keys in file:\")\n",
    "    for key in f.keys():\n",
    "        print(\"  \", key)\n",
    "\n",
    "    # Recursively print structure\n",
    "    def print_structure(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"[DATASET] {name} - shape: {obj.shape}, dtype: {obj.dtype}\")\n",
    "        elif isinstance(obj, h5py.Group):\n",
    "            print(f\"[GROUP] {name}\")\n",
    "\n",
    "    print(\"\\nFull structure:\")\n",
    "    f.visititems(print_structure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34cde5",
   "metadata": {},
   "source": [
    "Combine multiple tdportal reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72219622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined 6 files into F:\\20251115\\tdportal\\combined.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# --- Folder path containing your CSV files ---\n",
    "folder = r\"F:\\20251115\\tdportal\"   # ⬅️ change this to your actual folder path\n",
    "\n",
    "# --- Find all CSV files in the folder ---\n",
    "csv_files = glob.glob(os.path.join(folder, \"*.csv\"))\n",
    "\n",
    "# --- Read each CSV and combine them by row ---\n",
    "dfs = [pd.read_csv(f) for f in csv_files]\n",
    "combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# --- Save the combined CSV ---\n",
    "output_path = os.path.join(folder, \"combined.csv\")\n",
    "combined.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Combined {len(csv_files)} files into {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID_import(tdportal, databank, cast_path):\n",
    "  def str_to_int(st):\n",
    "      internal = []\n",
    "      digits = re.findall(r'\\d+', st)\n",
    "      for i in range(0, len(digits)):\n",
    "          internal.append(int(digits[i]))\n",
    "      return(internal)\n",
    "\n",
    "  scan_number = [0]*len(tdportal['File Name'])\n",
    "  td_samples = []\n",
    "\n",
    "  for i in range(0, len(tdportal['File Name'])):\n",
    "      scan_number[i] = str_to_int(str(tdportal['Fragment Scans'][i]))\n",
    "      if tdportal['File Name'][i] not in td_samples:\n",
    "        td_samples.append(tdportal['File Name'][i])\n",
    "\n",
    "  my_dic_scan = {key: [] for key in td_samples}\n",
    "  my_dic_index = {key: [] for key in td_samples}\n",
    "\n",
    "  for i in range(0, len(tdportal['File Name'])):\n",
    "      my_dic_scan[tdportal['File Name'][i]].append(scan_number[i])\n",
    "      my_dic_index[tdportal['File Name'][i]].append([i]*len(scan_number[i]))\n",
    "\n",
    "  for i in range(0, len(td_samples)):\n",
    "      nested_list = my_dic_scan[td_samples[i]]\n",
    "      flat_list = []\n",
    "      for item in nested_list:\n",
    "          if isinstance(item, list):\n",
    "              flat_list.extend(item)\n",
    "          else:\n",
    "              flat_list.append(item)\n",
    "      my_dic_scan[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "\n",
    "  for i in range(0, len(td_samples)):\n",
    "      nested_list = my_dic_index[td_samples[i]]\n",
    "      flat_list = []\n",
    "      for item in nested_list:\n",
    "          if isinstance(item, list):\n",
    "              flat_list.extend(item)\n",
    "          else:\n",
    "              flat_list.append(item)\n",
    "      my_dic_index[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "  sequence, MASS, Accession, missing, PFR = [], [], [], [], []\n",
    "\n",
    "  for i in tqdm(range(len(databank['scan'])), desc=\"Processing scans\", ncols=100):\n",
    "      try:\n",
    "          sample = databank['sample_name'][i]\n",
    "          scan   = databank['scan'][i]\n",
    "\n",
    "          if scan in my_dic_scan[sample]:\n",
    "              tt = my_dic_index[sample][my_dic_scan[sample].index(scan)]\n",
    "              sequence.append(tdportal.at[tt, 'Sequence'])\n",
    "              MASS.append(tdportal.at[tt, 'Average Mass'])\n",
    "              Accession.append(tdportal.at[tt, 'Accession'])\n",
    "              PFR.append(tdportal.at[tt, 'PFR'])\n",
    "          else:\n",
    "              sequence.append(None)\n",
    "              MASS.append(None)\n",
    "              Accession.append(None)\n",
    "              PFR.append(None)\n",
    "\n",
    "      except KeyError as e:\n",
    "          missing.append(sample)\n",
    "        # Handles missing sample_name or missing index key\n",
    "        # You could also log: print(f\"Missing key: {e}\")\n",
    "          sequence.append(None)\n",
    "          MASS.append(None)\n",
    "          Accession.append(None)\n",
    "          PFR.append(None)\n",
    "\n",
    "      except Exception as e:\n",
    "        # Catches other unexpected issues (out-of-range, missing column, etc.)\n",
    "        # print(f\"Unexpected error: {e}\")\n",
    "          sequence.append(None)\n",
    "          MASS.append(None)\n",
    "          Accession.append(None)\n",
    "          PFR.append(None)\n",
    "\n",
    "  print(set(missing))\n",
    "\n",
    "  databank['sequence'] = sequence\n",
    "  databank['MASS'] = MASS\n",
    "  databank['Accession'] = Accession\n",
    "  databank['PFR'] = PFR\n",
    "\n",
    "  databank = pd.DataFrame(databank)\n",
    "\n",
    "  databank.to_csv(cast_path, index=False)\n",
    "\n",
    "  return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19fc10f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 input files:\n",
      "  - F:\\20251115\\spectra_h5\\ms2_plasma_dataset.h5\n",
      "  - F:\\20251115\\spectra_h5\\ms2_tissue_dataset.h5\n",
      "Total rows across all files: 2202567\n",
      "Created output file: F:\\20251115\\spectra_h5/combined.h5\n",
      "Processing F:\\20251115\\spectra_h5\\ms2_plasma_dataset.h5 with 1219397 rows (writing from offset 0)\n",
      "Processing F:\\20251115\\spectra_h5\\ms2_tissue_dataset.h5 with 983170 rows (writing from offset 1219397)\n",
      "Done. Total rows written: 2202567\n",
      "All data copied successfully.\n",
      "Output file closed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Combine multiple HDF5 files with identical structure into a single compressed HDF5.\n",
    "\n",
    "Expected datasets in each input file:\n",
    "  - file_name      (N,)        |S59\n",
    "  - group_name     (N,)        |S6\n",
    "  - ms2_lib        (N, 1600)   float16\n",
    "  - precursor_mz   (N,)        float32\n",
    "  - rt_min         (N,)        float32\n",
    "  - scan           (N,)        int32\n",
    "\n",
    "Output:\n",
    "  - One HDF5 with the same dataset names, concatenated along axis 0,\n",
    "    stored with gzip compression + shuffle + chunking (npz-like behavior).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# User parameters\n",
    "# -----------------------------\n",
    "INPUT_DIR = r\"F:\\20251115\\spectra_h5\"      # <-- change this\n",
    "OUTPUT_FILE = r\"F:\\20251115\\spectra_h5/combined.h5\"    # <-- change this\n",
    "PATTERN = \"*.h5\"                         # which input files to combine\n",
    "\n",
    "# I/O chunk size when reading/copying (None = whole file at once; use e.g. 100_000 for huge datasets)\n",
    "COPY_CHUNK_ROWS = 100_000\n",
    "\n",
    "# HDF5 dataset compression / chunking\n",
    "H5_COMPRESSION = \"gzip\"     # \"gzip\" is closest to npz; you can use \"lzf\" for faster but less tight\n",
    "H5_COMP_OPTS = 4            # 1–9 for gzip, higher = more compression, slower\n",
    "H5_ROW_CHUNK = 4096         # chunk length along the row axis for HDF5 datasets\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def get_files(input_dir, pattern=\"*.h5\"):\n",
    "    files = sorted(glob.glob(os.path.join(input_dir, pattern)))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files found in {input_dir!r} matching {pattern!r}\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def inspect_first_file(example_file):\n",
    "    \"\"\"\n",
    "    Read the first file and capture shapes/dtypes to define the output file.\n",
    "    Also verifies that required datasets exist.\n",
    "    \"\"\"\n",
    "    expected_keys = [\"file_name\", \"group_name\", \"ms2_lib\",\n",
    "                     \"precursor_mz\", \"rt_min\", \"scan\"]\n",
    "    info = {}\n",
    "\n",
    "    with h5py.File(example_file, \"r\") as f:\n",
    "        for k in expected_keys:\n",
    "            if k not in f:\n",
    "                raise KeyError(f\"Dataset {k!r} not found in {example_file}\")\n",
    "            ds = f[k]\n",
    "            info[k] = {\n",
    "                \"shape\": ds.shape,\n",
    "                \"dtype\": ds.dtype,\n",
    "            }\n",
    "\n",
    "        # quick consistency checks\n",
    "        n = info[\"file_name\"][\"shape\"][0]\n",
    "        for k in expected_keys:\n",
    "            if f[k].shape[0] != n:\n",
    "                raise ValueError(\n",
    "                    f\"Dataset {k!r} in {example_file!r} has first dim \"\n",
    "                    f\"{f[k].shape[0]} != file_name first dim {n}\"\n",
    "                )\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def compute_total_rows(files):\n",
    "    \"\"\"\n",
    "    Sum up rows across all files and sanity-check that\n",
    "    each file has internally consistent first dimensions.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for fp in files:\n",
    "        with h5py.File(fp, \"r\") as f:\n",
    "            n = f[\"file_name\"].shape[0]\n",
    "            assert f[\"group_name\"].shape[0] == n\n",
    "            assert f[\"ms2_lib\"].shape[0] == n\n",
    "            assert f[\"precursor_mz\"].shape[0] == n\n",
    "            assert f[\"rt_min\"].shape[0] == n\n",
    "            assert f[\"scan\"].shape[0] == n\n",
    "        total += n\n",
    "    return total\n",
    "\n",
    "\n",
    "def create_output_file(output_file, first_info, total_rows,\n",
    "                       compression=H5_COMPRESSION,\n",
    "                       compression_opts=H5_COMP_OPTS,\n",
    "                       row_chunk=H5_ROW_CHUNK):\n",
    "    \"\"\"\n",
    "    Create the output HDF5 with compressed, chunked datasets.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "\n",
    "    ms2_dim = first_info[\"ms2_lib\"][\"shape\"][1]\n",
    "\n",
    "    f_out = h5py.File(output_file, \"w\")\n",
    "\n",
    "    # Chunk shapes\n",
    "    chunk_scalar = (row_chunk,)\n",
    "    chunk_ms2 = (row_chunk, ms2_dim)\n",
    "\n",
    "    d_file_name = f_out.create_dataset(\n",
    "        \"file_name\",\n",
    "        shape=(total_rows,),\n",
    "        dtype=first_info[\"file_name\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_scalar,\n",
    "    )\n",
    "\n",
    "    d_group_name = f_out.create_dataset(\n",
    "        \"group_name\",\n",
    "        shape=(total_rows,),\n",
    "        dtype=first_info[\"group_name\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_scalar,\n",
    "    )\n",
    "\n",
    "    d_ms2_lib = f_out.create_dataset(\n",
    "        \"ms2_lib\",\n",
    "        shape=(total_rows, ms2_dim),\n",
    "        dtype=first_info[\"ms2_lib\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_ms2,\n",
    "    )\n",
    "\n",
    "    d_precursor_mz = f_out.create_dataset(\n",
    "        \"precursor_mz\",\n",
    "        shape=(total_rows,),\n",
    "        dtype=first_info[\"precursor_mz\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_scalar,\n",
    "    )\n",
    "\n",
    "    d_rt_min = f_out.create_dataset(\n",
    "        \"rt_min\",\n",
    "        shape=(total_rows,),\n",
    "        dtype=first_info[\"rt_min\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_scalar,\n",
    "    )\n",
    "\n",
    "    d_scan = f_out.create_dataset(\n",
    "        \"scan\",\n",
    "        shape=(total_rows,),\n",
    "        dtype=first_info[\"scan\"][\"dtype\"],\n",
    "        compression=compression,\n",
    "        compression_opts=compression_opts,\n",
    "        shuffle=True,\n",
    "        chunks=chunk_scalar,\n",
    "    )\n",
    "\n",
    "    dsets_out = {\n",
    "        \"file_name\": d_file_name,\n",
    "        \"group_name\": d_group_name,\n",
    "        \"ms2_lib\": d_ms2_lib,\n",
    "        \"precursor_mz\": d_precursor_mz,\n",
    "        \"rt_min\": d_rt_min,\n",
    "        \"scan\": d_scan,\n",
    "    }\n",
    "\n",
    "    return f_out, dsets_out\n",
    "\n",
    "\n",
    "def copy_data(files, dsets_out, chunk_rows=None):\n",
    "    \"\"\"\n",
    "    Concatenate datasets from all input files into output datasets along axis 0.\n",
    "\n",
    "    If chunk_rows is None:\n",
    "        - copy each file in one go (simple, more RAM).\n",
    "    Else:\n",
    "        - copy in slices of size chunk_rows (safer for huge files).\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "\n",
    "    for fp in files:\n",
    "        with h5py.File(fp, \"r\") as f_in:\n",
    "            n_rows = f_in[\"file_name\"].shape[0]\n",
    "            print(f\"Processing {fp} with {n_rows} rows (writing from offset {offset})\")\n",
    "\n",
    "            if chunk_rows is None:\n",
    "                # Whole file at once\n",
    "                sl = slice(offset, offset + n_rows)\n",
    "                dsets_out[\"file_name\"][sl] = f_in[\"file_name\"][...]\n",
    "                dsets_out[\"group_name\"][sl] = f_in[\"group_name\"][...]\n",
    "                dsets_out[\"ms2_lib\"][sl] = f_in[\"ms2_lib\"][...]\n",
    "                dsets_out[\"precursor_mz\"][sl] = f_in[\"precursor_mz\"][...]\n",
    "                dsets_out[\"rt_min\"][sl] = f_in[\"rt_min\"][...]\n",
    "                dsets_out[\"scan\"][sl] = f_in[\"scan\"][...]\n",
    "            else:\n",
    "                # Chunked copy\n",
    "                for start in range(0, n_rows, chunk_rows):\n",
    "                    end = min(start + chunk_rows, n_rows)\n",
    "                    local_sl = slice(start, end)\n",
    "                    global_sl = slice(offset + start, offset + end)\n",
    "\n",
    "                    dsets_out[\"file_name\"][global_sl] = f_in[\"file_name\"][local_sl]\n",
    "                    dsets_out[\"group_name\"][global_sl] = f_in[\"group_name\"][local_sl]\n",
    "                    dsets_out[\"ms2_lib\"][global_sl] = f_in[\"ms2_lib\"][local_sl]\n",
    "                    dsets_out[\"precursor_mz\"][global_sl] = f_in[\"precursor_mz\"][local_sl]\n",
    "                    dsets_out[\"rt_min\"][global_sl] = f_in[\"rt_min\"][local_sl]\n",
    "                    dsets_out[\"scan\"][global_sl] = f_in[\"scan\"][local_sl]\n",
    "\n",
    "        offset += n_rows\n",
    "\n",
    "    print(f\"Done. Total rows written: {offset}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    files = get_files(INPUT_DIR, PATTERN)\n",
    "    print(f\"Found {len(files)} input files:\")\n",
    "    for f in files:\n",
    "        print(\"  -\", f)\n",
    "\n",
    "    first_info = inspect_first_file(files[0])\n",
    "    total_rows = compute_total_rows(files)\n",
    "    print(f\"Total rows across all files: {total_rows}\")\n",
    "\n",
    "    f_out, dsets_out = create_output_file(OUTPUT_FILE, first_info, total_rows)\n",
    "    print(f\"Created output file: {OUTPUT_FILE}\")\n",
    "\n",
    "    try:\n",
    "        copy_data(files, dsets_out, chunk_rows=COPY_CHUNK_ROWS)\n",
    "        print(\"All data copied successfully.\")\n",
    "    finally:\n",
    "        f_out.close()\n",
    "        print(\"Output file closed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b690439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['file_name', 'group_name', 'ms2_lib', 'precursor_mz', 'rt_min', 'scan']\n",
      "[file_name] shape=(2202567,), dtype=object\n",
      "[group_name] shape=(2202567,), dtype=object\n",
      "[ms2_lib] shape=(2202567, 1600), dtype=float16\n",
      "[precursor_mz] shape=(2202567,), dtype=float32\n",
      "[rt_min] shape=(2202567,), dtype=float32\n",
      "[scan] shape=(2202567,), dtype=int64\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "path = r\"F:\\20251115\\spectra_h5\\combined.h5\"   # <-- change this to your file name\n",
    "\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    print(\"Keys:\", list(f.keys()))\n",
    "    \n",
    "    for key in f.keys():\n",
    "        dset = f[key]\n",
    "        print(f\"[{key}] shape={dset.shape}, dtype={dset.dtype}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
